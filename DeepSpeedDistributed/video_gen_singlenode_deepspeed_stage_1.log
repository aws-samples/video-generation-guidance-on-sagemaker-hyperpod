/var/spool/slurmd/job00015/slurm_script: line 9: LICENSE: command not found
/var/spool/slurmd/job00015/slurm_script: line 11: LICENSE: command not found
[2024-07-03 07:07:34,521] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'
  warnings.warn(
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'
  warnings.warn(
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'
  warnings.warn(
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'
  warnings.warn(
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
/fsx/ubuntu/miniconda3/envs/videogen/lib/python3.10/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  return register_model(fn_wrapper)
[2024-07-03 07:07:51,217] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-03 07:07:51,217] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-03 07:07:51,238] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-03 07:07:51,238] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[2024-07-03 07:07:54,831] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-03 07:07:54,836] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-03 07:07:54,836] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-03 07:07:54,854] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-03 07:07:54,865] [INFO] [comm.py:637:init_distributed] cdb=None
07/03/2024 07:07:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}

07/03/2024 07:07:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: fp16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}

07/03/2024 07:07:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: fp16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}

07/03/2024 07:07:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: fp16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}

{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.
{'encoder_hid_dim', 'addition_time_embed_dim', 'conv_in_kernel', 'time_embedding_type', 'dropout', 'class_embeddings_concat', 'addition_embed_type_num_heads', 'resnet_out_scale_factor', 'time_embedding_act_fn', 'reverse_transformer_layers_per_block', 'time_cond_proj_dim', 'resnet_time_scale_shift', 'time_embedding_dim', 'num_attention_heads', 'addition_embed_type', 'cross_attention_norm', 'mid_block_only_cross_attention', 'projection_class_embeddings_input_dim', 'upcast_attention', 'class_embed_type', 'resnet_skip_time_act', 'timestep_post_act', 'encoder_hid_dim_type', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'transformer_layers_per_block'} was not found in config. Values will be initialized to default values.
Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: 
 ['conv_norm_out.weight, conv_norm_out.bias, conv_out.weight, conv_out.bias']
07/03/2024 07:08:03 - INFO - src.models.unet_3d - loaded temporal unet's pretrained weights from pretrained_weights/sd-image-variations-diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/42bc0ee1726b141d49f519a6ea02ccfbf073db2e/unet ...
07/03/2024 07:08:03 - INFO - src.models.unet_3d - loaded temporal unet's pretrained weights from pretrained_weights/sd-image-variations-diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/42bc0ee1726b141d49f519a6ea02ccfbf073db2e/unet ...
07/03/2024 07:08:03 - INFO - src.models.unet_3d - loaded temporal unet's pretrained weights from pretrained_weights/sd-image-variations-diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/42bc0ee1726b141d49f519a6ea02ccfbf073db2e/unet ...
{'motion_module_decoder_only', 'mode', 'resnet_time_scale_shift', 'motion_module_resolutions', 'upcast_attention', 'class_embed_type', 'motion_module_mid_block', 'motion_module_kwargs', 'task_type', 'motion_module_type', 'unet_use_cross_frame_attention', 'use_inflated_groupnorm'} was not found in config. Values will be initialized to default values.
07/03/2024 07:08:03 - INFO - src.models.unet_3d - loaded temporal unet's pretrained weights from pretrained_weights/sd-image-variations-diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/42bc0ee1726b141d49f519a6ea02ccfbf073db2e/unet ...
07/03/2024 07:08:10 - INFO - src.models.unet_3d - Loaded 0.0M-parameter motion module
07/03/2024 07:08:10 - INFO - src.models.unet_3d - Loaded 0.0M-parameter motion module
07/03/2024 07:08:10 - INFO - src.models.unet_3d - Loaded 0.0M-parameter motion module
07/03/2024 07:08:10 - INFO - src.models.unet_3d - Loaded 0.0M-parameter motion module
07/03/2024 07:08:17 - INFO - __main__ - Missing key for pose guider: 2
[2024-07-03 07:08:18,176] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.3, git-hash=unknown, git-branch=unknown
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/03/2024 07:08:18 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
ip-10-1-17-43:297244:297244 [0] NCCL INFO Bootstrap : Using ens6:10.1.17.43<0>
ip-10-1-17-43:297244:297244 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
ip-10-1-17-43:297244:297244 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
ip-10-1-17-43:297244:297244 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.14.3+cuda11.7
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Using Libfabric version 1.21
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Using CUDA driver version 12020
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Configuring AWS-specific options
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Setting provider_filter to efa
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Internode latency set at 150.0 us
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
ip-10-1-17-43:297246:297246 [2] NCCL INFO cudaDriverVersion 12020
ip-10-1-17-43:297247:297247 [3] NCCL INFO cudaDriverVersion 12020
ip-10-1-17-43:297245:297245 [1] NCCL INFO cudaDriverVersion 12020
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Setting NCCL_PROTO to "simple"
ip-10-1-17-43:297244:297392 [0] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297244:297392 [0] NCCL INFO DMA-BUF is available on GPU device 0
ip-10-1-17-43:297246:297246 [2] NCCL INFO Bootstrap : Using ens6:10.1.17.43<0>
ip-10-1-17-43:297246:297246 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
ip-10-1-17-43:297246:297246 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Using Libfabric version 1.21
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Using CUDA driver version 12020
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Configuring AWS-specific options
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Setting provider_filter to efa
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Internode latency set at 150.0 us
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
ip-10-1-17-43:297245:297245 [1] NCCL INFO Bootstrap : Using ens6:10.1.17.43<0>
ip-10-1-17-43:297245:297245 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
ip-10-1-17-43:297245:297245 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Using Libfabric version 1.21
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Using CUDA driver version 12020
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Configuring AWS-specific options
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Setting provider_filter to efa
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
ip-10-1-17-43:297247:297247 [3] NCCL INFO Bootstrap : Using ens6:10.1.17.43<0>
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Internode latency set at 150.0 us
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
ip-10-1-17-43:297247:297247 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
ip-10-1-17-43:297247:297247 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Using Libfabric version 1.21
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Using CUDA driver version 12020
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Configuring AWS-specific options
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Setting provider_filter to efa
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Internode latency set at 150.0 us
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Setting NCCL_PROTO to "simple"
ip-10-1-17-43:297246:297393 [2] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297246:297393 [2] NCCL INFO DMA-BUF is available on GPU device 2
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Setting NCCL_PROTO to "simple"
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Setting NCCL_PROTO to "simple"
ip-10-1-17-43:297245:297394 [1] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297245:297394 [1] NCCL INFO DMA-BUF is available on GPU device 1
ip-10-1-17-43:297247:297395 [3] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297247:297395 [3] NCCL INFO DMA-BUF is available on GPU device 3
ip-10-1-17-43:297246:297393 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297247:297395 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297244:297392 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297245:297394 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297246:297393 [2] NCCL INFO NCCL_IGNORE_DISABLED_P2P set by environment to 1.
ip-10-1-17-43:297247:297395 [3] NCCL INFO NCCL_IGNORE_DISABLED_P2P set by environment to 1.
ip-10-1-17-43:297245:297394 [1] NCCL INFO NCCL_IGNORE_DISABLED_P2P set by environment to 1.
ip-10-1-17-43:297244:297392 [0] NCCL INFO NCCL_IGNORE_DISABLED_P2P set by environment to 1.
ip-10-1-17-43:297247:297395 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
ip-10-1-17-43:297244:297392 [0] NCCL INFO Channel 00/02 :    0   1   2   3
ip-10-1-17-43:297246:297393 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
ip-10-1-17-43:297245:297394 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
ip-10-1-17-43:297244:297392 [0] NCCL INFO Channel 01/02 :    0   1   2   3
ip-10-1-17-43:297244:297392 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
ip-10-1-17-43:297247:297395 [3] NCCL INFO Channel 00 : 3[1e0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297246:297393 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct
ip-10-1-17-43:297247:297395 [3] NCCL INFO Channel 01 : 3[1e0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297246:297393 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct
ip-10-1-17-43:297245:297394 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297244:297392 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297245:297394 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297244:297392 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297247:297395 [3] NCCL INFO Connected all rings
ip-10-1-17-43:297246:297393 [2] NCCL INFO Connected all rings
ip-10-1-17-43:297247:297395 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297247:297395 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297245:297394 [1] NCCL INFO Connected all rings
ip-10-1-17-43:297244:297392 [0] NCCL INFO Connected all rings
ip-10-1-17-43:297246:297393 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297246:297393 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297245:297394 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297247:297395 [3] NCCL INFO Connected all trees
ip-10-1-17-43:297247:297395 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297247:297395 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297245:297394 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297244:297392 [0] NCCL INFO Connected all trees
ip-10-1-17-43:297244:297392 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297244:297392 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297246:297393 [2] NCCL INFO Connected all trees
ip-10-1-17-43:297246:297393 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297246:297393 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297245:297394 [1] NCCL INFO Connected all trees
ip-10-1-17-43:297245:297394 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297245:297394 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297246:297393 [2] NCCL INFO comm 0x1c101f60 rank 2 nranks 4 cudaDev 2 busId 1d0 - Init COMPLETE
ip-10-1-17-43:297247:297395 [3] NCCL INFO comm 0x1922fb10 rank 3 nranks 4 cudaDev 3 busId 1e0 - Init COMPLETE
ip-10-1-17-43:297244:297392 [0] NCCL INFO comm 0x1390fea0 rank 0 nranks 4 cudaDev 0 busId 1b0 - Init COMPLETE
ip-10-1-17-43:297245:297394 [1] NCCL INFO comm 0x109363c0 rank 1 nranks 4 cudaDev 1 busId 1c0 - Init COMPLETE
[2024-07-03 07:08:19,048] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-03 07:08:19,051] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-03 07:08:19,051] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-03 07:08:19,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-03 07:08:19,271] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-03 07:08:19,271] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-07-03 07:08:19,271] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2024-07-03 07:08:19,271] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2024-07-03 07:08:19,272] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-07-03 07:08:19,272] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-07-03 07:08:22,813] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-07-03 07:08:22,814] [INFO] [utils.py:782:see_memory_usage] MA 5.52 GB         Max_MA 6.31 GB         CA 6.81 GB         Max_CA 7 GB 
[2024-07-03 07:08:22,814] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.88 GB, percent = 12.8%
[2024-07-03 07:08:22,948] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-07-03 07:08:22,949] [INFO] [utils.py:782:see_memory_usage] MA 5.52 GB         Max_MA 7.1 GB         CA 8.39 GB         Max_CA 8 GB 
[2024-07-03 07:08:22,949] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.88 GB, percent = 12.8%
[2024-07-03 07:08:22,949] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-07-03 07:08:23,082] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-07-03 07:08:23,083] [INFO] [utils.py:782:see_memory_usage] MA 5.52 GB         Max_MA 5.52 GB         CA 8.39 GB         Max_CA 8 GB 
[2024-07-03 07:08:23,083] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.88 GB, percent = 12.8%
[2024-07-03 07:08:23,090] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-07-03 07:08:23,090] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-03 07:08:23,090] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-07-03 07:08:23,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.999)]
[2024-07-03 07:08:23,092] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   amp_enabled .................. False
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   amp_params ................... False
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  False
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f048190ef80>
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   communication_data_type ...... None
[2024-07-03 07:08:23,093] [INFO] [config.py:1004:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   disable_allgather ............ False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   dump_state ................... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   elasticity_enabled ........... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   fp16_auto_cast ............... True
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   fp16_enabled ................. True
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   global_rank .................. 0
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 1
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.0
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   graph_harvesting ............. False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   loss_scale ................... 0
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   memory_breakdown ............. False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   mics_shard_size .............. -1
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   optimizer_name ............... None
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   optimizer_params ............. None
[2024-07-03 07:08:23,094] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   pld_enabled .................. False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   pld_params ................... False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   prescale_gradients ........... False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   scheduler_name ............... None
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   scheduler_params ............. None
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   sparse_attention ............. None
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   steps_per_print .............. inf
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   train_batch_size ............. 8
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  2
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   use_node_local_storage ....... False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   weight_quantization_config ... None
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   world_size ................... 4
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  True
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   zero_enabled ................. True
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-03 07:08:23,095] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2
[2024-07-03 07:08:23,095] [INFO] [config.py:990:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "fp16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
07/03/2024 07:08:23 - INFO - __main__ - ***** Running training *****
07/03/2024 07:08:23 - INFO - __main__ -   Num examples = 34
07/03/2024 07:08:23 - INFO - __main__ -   Num Epochs = 80
07/03/2024 07:08:23 - INFO - __main__ -   Instantaneous batch size per device = 2
07/03/2024 07:08:23 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
07/03/2024 07:08:23 - INFO - __main__ -   Gradient Accumulation steps = 1
07/03/2024 07:08:23 - INFO - __main__ -   Total optimization steps = 400
ip-10-1-17-43:297244:297413 [0] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297244:297413 [0] NCCL INFO DMA-BUF is available on GPU device 0
ip-10-1-17-43:297247:297414 [3] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297246:297415 [2] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297247:297414 [3] NCCL INFO DMA-BUF is available on GPU device 3
ip-10-1-17-43:297245:297416 [1] NCCL INFO Using network AWS Libfabric
ip-10-1-17-43:297246:297415 [2] NCCL INFO DMA-BUF is available on GPU device 2
ip-10-1-17-43:297245:297416 [1] NCCL INFO DMA-BUF is available on GPU device 1
ip-10-1-17-43:297245:297416 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297247:297414 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297246:297415 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297244:297413 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains
ip-10-1-17-43:297245:297416 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
ip-10-1-17-43:297246:297415 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
ip-10-1-17-43:297244:297413 [0] NCCL INFO Channel 00/02 :    0   1   2   3
ip-10-1-17-43:297247:297414 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
ip-10-1-17-43:297244:297413 [0] NCCL INFO Channel 01/02 :    0   1   2   3
ip-10-1-17-43:297244:297413 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
ip-10-1-17-43:297247:297414 [3] NCCL INFO Channel 00 : 3[1e0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297245:297416 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297247:297414 [3] NCCL INFO Channel 01 : 3[1e0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297245:297416 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297246:297415 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct
ip-10-1-17-43:297244:297413 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297246:297415 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct
ip-10-1-17-43:297244:297413 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297247:297414 [3] NCCL INFO Connected all rings
ip-10-1-17-43:297245:297416 [1] NCCL INFO Connected all rings
ip-10-1-17-43:297247:297414 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297247:297414 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct
ip-10-1-17-43:297246:297415 [2] NCCL INFO Connected all rings
ip-10-1-17-43:297244:297413 [0] NCCL INFO Connected all rings
ip-10-1-17-43:297245:297416 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297245:297416 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct
ip-10-1-17-43:297246:297415 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297246:297415 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct
ip-10-1-17-43:297244:297413 [0] NCCL INFO Connected all trees
ip-10-1-17-43:297244:297413 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297244:297413 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297247:297414 [3] NCCL INFO Connected all trees
ip-10-1-17-43:297247:297414 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297247:297414 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297245:297416 [1] NCCL INFO Connected all trees
ip-10-1-17-43:297245:297416 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297245:297416 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297246:297415 [2] NCCL INFO Connected all trees
ip-10-1-17-43:297246:297415 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ip-10-1-17-43:297246:297415 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ip-10-1-17-43:297244:297413 [0] NCCL INFO comm 0xfeec8f0 rank 0 nranks 4 cudaDev 0 busId 1b0 - Init COMPLETE
ip-10-1-17-43:297245:297416 [1] NCCL INFO comm 0x1a2769c0 rank 1 nranks 4 cudaDev 1 busId 1c0 - Init COMPLETE
ip-10-1-17-43:297246:297415 [2] NCCL INFO comm 0xf0eed6b0 rank 2 nranks 4 cudaDev 2 busId 1d0 - Init COMPLETE
ip-10-1-17-43:297247:297414 [3] NCCL INFO comm 0x16e01a80 rank 3 nranks 4 cudaDev 3 busId 1e0 - Init COMPLETE
[2024-07-03 07:08:33,560] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-07-03 07:08:36,205] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-07-03 07:08:38,846] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-07-03 07:08:41,488] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-07-03 07:08:44,305] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-07-03 07:08:48,168] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
  0%|          | 0/400 [00:00<?, ?it/s]Steps:   0%|          | 0/400 [00:00<?, ?it/s]Steps:   0%|          | 1/400 [00:10<1:07:20, 10.13s/it]Steps:   0%|          | 1/400 [00:10<1:07:20, 10.13s/it, lr=1e-5, step_loss=1.16]Steps:   0%|          | 2/400 [00:12<37:59,  5.73s/it, lr=1e-5, step_loss=1.16]  Steps:   0%|          | 2/400 [00:12<37:59,  5.73s/it, lr=1e-5, step_loss=2.04]Steps:   1%|          | 3/400 [00:15<28:34,  4.32s/it, lr=1e-5, step_loss=2.04]Steps:   1%|          | 3/400 [00:15<28:34,  4.32s/it, lr=1e-5, step_loss=1.03]Steps:   1%|          | 4/400 [00:18<24:07,  3.66s/it, lr=1e-5, step_loss=1.03]Steps:   1%|          | 4/400 [00:18<24:07,  3.66s/it, lr=1e-5, step_loss=1.58]Steps:   1%|▏         | 5/400 [00:20<22:04,  3.35s/it, lr=1e-5, step_loss=1.58]Steps:   1%|▏         | 5/400 [00:20<22:04,  3.35s/it, lr=1e-5, step_loss=1.42]Steps:   2%|▏         | 6/400 [00:24<23:09,  3.53s/it, lr=1e-5, step_loss=1.42]Steps:   2%|▏         | 6/400 [00:24<23:09,  3.53s/it, lr=1e-5, ste[2024-07-03 07:08:50,810] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-07-03 07:08:53,454] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-07-03 07:08:56,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-07-03 07:08:58,886] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
[2024-07-03 07:09:02,843] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
[2024-07-03 07:09:05,482] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
[2024-07-03 07:09:08,130] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
p_loss=2.74]Steps:   2%|▏         | 7/400 [00:27<21:12,  3.24s/it, lr=1e-5, step_loss=2.74]Steps:   2%|▏         | 7/400 [00:27<21:12,  3.24s/it, lr=1e-5, step_loss=2.21]Steps:   2%|▏         | 8/400 [00:30<19:55,  3.05s/it, lr=1e-5, step_loss=2.21]Steps:   2%|▏         | 8/400 [00:30<19:55,  3.05s/it, lr=1e-5, step_loss=0.303]Steps:   2%|▏         | 9/400 [00:32<19:03,  2.92s/it, lr=1e-5, step_loss=0.303]Steps:   2%|▏         | 9/400 [00:32<19:03,  2.92s/it, lr=1e-5, step_loss=2.22] Steps:   2%|▎         | 10/400 [00:35<18:43,  2.88s/it, lr=1e-5, step_loss=2.22]Steps:   2%|▎         | 10/400 [00:35<18:43,  2.88s/it, lr=1e-5, step_loss=1.08]Steps:   3%|▎         | 11/400 [00:39<20:48,  3.21s/it, lr=1e-5, step_loss=1.08]Steps:   3%|▎         | 11/400 [00:39<20:48,  3.21s/it, lr=1e-5, step_loss=1.78]Steps:   3%|▎         | 12/400 [00:42<19:37,  3.04s/it, lr=1e-5, step_loss=1.78]Steps:   3%|▎         | 12/400 [00:42<19:37,  3.04s/it, lr=1e-5, step_loss=2.29]Steps:   3%|▎   [2024-07-03 07:09:10,767] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
[2024-07-03 07:09:12,388] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2024-07-03 07:09:16,187] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
      | 13/400 [00:44<18:49,  2.92s/it, lr=1e-5, step_loss=2.29]Steps:   3%|▎         | 13/400 [00:44<18:49,  2.92s/it, lr=1e-5, step_loss=2.33]Steps:   4%|▎         | 14/400 [00:47<18:13,  2.83s/it, lr=1e-5, step_loss=2.33]Steps:   4%|▎         | 14/400 [00:47<18:13,  2.83s/it, lr=1e-5, step_loss=2.29]Steps:   4%|▍         | 15/400 [00:48<15:50,  2.47s/it, lr=1e-5, step_loss=2.29]Steps:   4%|▍         | 15/400 [00:48<15:50,  2.47s/it, lr=1e-5, step_loss=1.46]Steps:   4%|▍         | 16/400 [00:52<18:21,  2.87s/it, lr=1e-5, step_loss=1.46]Steps:   4%|▍         | 16/400 [00:52<18:21,  2.87s/it, lr=1e-5, step_loss=2.41]Steps:   4%|▍         | 17/400 [00:58<22:58,  3.60s/it, lr=1e-5, step_loss=2.41]Steps:   4%|▍         | 17/400 [00:58<22:58,  3.60s/it, lr=1e-5, step_loss=2.3] Steps:   4%|▍         | 18/400 [01:03<26:23,  4.15s/it, lr=1e-5, step_loss=2.3]Steps:   4%|▍         | 18/400 [01:03<26:23,  4.15s/it, lr=1e-5, step_loss=0.995]Steps:   5%|▍         | 19/400 [01:08<28:15,  4.45s/it, lr=1e-5, step_loss=0.995]Steps:   5%|▍         | 19/400 [01:08<28:15,  4.45s/it, lr=1e-5, step_loss=0.876]Steps:   5%|▌         | 20/400 [01:13<29:24,  4.64s/it, lr=1e-5, step_loss=0.876]Steps:   5%|▌         | 20/400 [01:13<29:24,  4.64s/it, lr=1e-5, step_loss=1.77] Steps:   5%|▌         | 21/400 [01:19<32:23,  5.13s/it, lr=1e-5, step_loss=1.77]Steps:   5%|▌         | 21/400 [01:19<32:23,  5.13s/it, lr=1e-5, step_loss=1.88]Steps:   6%|▌         | 22/400 [01:24<32:01,  5.08s/it, lr=1e-5, step_loss=1.88]Steps:   6%|▌         | 22/400 [01:24<32:01,  5.08s/it, lr=1e-5, step_loss=0.817]Steps:   6%|▌         | 23/400 [01:29<31:47,  5.06s/it, lr=1e-5, step_loss=0.817]Steps:   6%|▌         | 23/400 [01:29<31:47,  5.06s/it, lr=1e-5, step_loss=1.17] Steps:   6%|▌         | 24/400 [01:34<31:27,  5.02s/it, lr=1e-5, step_loss=1.17]Steps:   6%|▌         | 24/400 [01:34<31:27,  5.02s/it, lr=1e-5, step_loss=1.75]Steps:   6%|▋         | 25/400 [01:40<31:38,  5.06s/it, lr=1e-5, step_loss=1.75]Steps:   6%|▋         | 25/400 [01:40<31:38,  5.06s/it, lr=1e-5, step_loss=0.396]Steps:   6%|▋         | 26/400 [01:46<34:03,  5.46s/it, lr=1e-5, step_loss=0.396]Steps:   6%|▋         | 26/400 [01:46<34:03,  5.46s/it, lr=1e-5, step_loss=0.642]Steps:   7%|▋         | 27/400 [01:50<30:49,  4.96s/it, lr=1e-5, step_loss=0.642]Steps:   7%|▋         | 27/400 [01:50<30:49,  4.96s/it, lr=1e-5, step_loss=0.637]Steps:   7%|▋         | 28/400 [01:55<30:47,  4.97s/it, lr=1e-5, step_loss=0.637]Steps:   7%|▋         | 28/400 [01:55<30:47,  4.97s/it, lr=1e-5, step_loss=0.924]Steps:   7%|▋         | 29/400 [02:00<30:41,  4.96s/it, lr=1e-5, step_loss=0.924]Steps:   7%|▋         | 29/400 [02:00<30:41,  4.96s/it, lr=1e-5, step_loss=1.91] Steps:   8%|▊         | 30/400 [02:05<31:08,  5.05s/it, lr=1e-5, step_loss=1.91]Steps:   8%|▊         | 30/400 [02:05<31:08,  5.05s/it, lr=1e-5, step_loss=1.69]Steps:   8%|▊         | 31/400 [02:11<33:12,  5.40s/it, lr=1e-5, step_loss=1.69]Steps:   8%|▊         | 31/400 [02:11<33:12,  5.40s/it, lr=1e-5, step_loss=1.05]Steps:   8%|▊         | 32/400 [02:16<32:21,  5.28s/it, lr=1e-5, step_loss=1.05]Steps:   8%|▊         | 32/400 [02:16<32:21,  5.28s/it, lr=1e-5, step_loss=0.411]Steps:   8%|▊         | 33/400 [02:21<32:02,  5.24s/it, lr=1e-5, step_loss=0.411]Steps:   8%|▊         | 33/400 [02:21<32:02,  5.24s/it, lr=1e-5, step_loss=0.851]Steps:   8%|▊         | 34/400 [02:26<31:29,  5.16s/it, lr=1e-5, step_loss=0.851]Steps:   8%|▊         | 34/400 [02:26<31:29,  5.16s/it, lr=1e-5, step_loss=0.898]Steps:   9%|▉         | 35/400 [02:31<31:24,  5.16s/it, lr=1e-5, step_loss=0.898]Steps:   9%|▉         | 35/400 [02:31<31:24,  5.16s/it, lr=1e-5, step_loss=0.889]Steps:   9%|▉         | 36/400 [02:38<33:24,  5.51s/it, lr=1e-5, step_loss=0.889]Steps:   9%|▉         | 36/400 [02:38<33:24,  5.51s/it, lr=1e-5, step_loss=0.174]Steps:   9%|▉         | 37/400 [02:43<32:22,  5.35s/it, lr=1e-5, step_loss=0.174]Steps:   9%|▉         | 37/400 [02:43<32:22,  5.35s/it, lr=1e-5, step_loss=0.906]Steps:  10%|▉         | 38/400 [02:48<31:33,  5.23s/it, lr=1e-5, step_loss=0.906]Steps:  10%|▉         | 38/400 [02:48<31:33,  5.23s/it, lr=1e-5, step_loss=1.15] Steps:  10%|▉         | 39/400 [02:53<31:18,  5.20s/it, lr=1e-5, step_loss=1.15]Steps:  10%|▉         | 39/400 [02:53<31:18,  5.20s/it, lr=1e-5, step_loss=0.586]Steps:  10%|█         | 40/400 [02:58<30:59,  5.16s/it, lr=1e-5, step_loss=0.586]Steps:  10%|█         | 40/400 [02:58<30:59,  5.16s/it, lr=1e-5, step_loss=0.901]Steps:  10%|█         | 41/400 [03:04<32:33,  5.44s/it, lr=1e-5, step_loss=0.901]Steps:  10%|█         | 41/400 [03:04<32:33,  5.44s/it, lr=1e-5, step_loss=0.922]Steps:  10%|█         | 42/400 [03:09<31:57,  5.35s/it, lr=1e-5, step_loss=0.922]Steps:  10%|█         | 42/400 [03:09<31:57,  5.35s/it, lr=1e-5, step_loss=1.42] Steps:  11%|█         | 43/400 [03:14<31:07,  5.23s/it, lr=1e-5, step_loss=1.42]Steps:  11%|█         | 43/400 [03:14<31:07,  5.23s/it, lr=1e-5, step_loss=1.1] Steps:  11%|█         | 44/400 [03:18<28:33,  4.81s/it, lr=1e-5, step_loss=1.1]Steps:  11%|█         | 44/400 [03:18<28:33,  4.81s/it, lr=1e-5, step_loss=0.496]Steps:  11%|█▏        | 45/400 [03:23<29:00,  4.90s/it, lr=1e-5, step_loss=0.496]Steps:  11%|█▏        | 45/400 [03:23<29:00,  4.90s/it, lr=1e-5, step_loss=0.818]Steps:  12%|█▏        | 46/400 [03:29<31:34,  5.35s/it, lr=1e-5, step_loss=0.818]Steps:  12%|█▏        | 46/400 [03:29<31:34,  5.35s/it, lr=1e-5, step_loss=0.621]Steps:  12%|█▏        | 47/400 [03:33<28:43,  4.88s/it, lr=1e-5, step_loss=0.621]Steps:  12%|█▏        | 47/400 [03:33<28:43,  4.88s/it, lr=1e-5, step_loss=0.844]Steps:  12%|█▏        | 48/400 [03:38<28:50,  4.92s/it, lr=1e-5, step_loss=0.844]Steps:  12%|█▏        | 48/400 [03:38<28:50,  4.92s/it, lr=1e-5, step_loss=0.774]Steps:  12%|█▏        | 49/400 [03:43<28:53,  4.94s/it, lr=1e-5, step_loss=0.774]Steps:  12%|█▏        | 49/400 [03:43<28:53,  4.94s/it, lr=1e-5, step_loss=1.16] Steps:  12%|█▎        | 50/400 [03:48<29:24,  5.04s/it, lr=1e-5, step_loss=1.16]Steps:  12%|█▎        | 50/400 [03:48<29:24,  5.04s/it, lr=1e-5, step_loss=1.41]Steps:  13%|█▎        | 51/400 [03:55<31:26,  5.41s/it, lr=1e-5, step_loss=1.41]Steps:  13%|█▎        | 51/400 [03:55<31:26,  5.41s/it, lr=1e-5, step_loss=0.442]Steps:  13%|█▎        | 52/400 [04:00<30:35,  5.28s/it, lr=1e-5, step_loss=0.442]Steps:  13%|█▎        | 52/400 [04:00<30:35,  5.28s/it, lr=1e-5, step_loss=1.2]  Steps:  13%|█▎        | 53/400 [04:05<30:17,  5.24s/it, lr=1e-5, step_loss=1.2]Steps:  13%|█▎        | 53/400 [04:05<30:17,  5.24s/it, lr=1e-5, step_loss=0.61]Steps:  14%|█▎        | 54/400 [04:10<29:41,  5.15s/it, lr=1e-5, step_loss=0.61]Steps:  14%|█▎        | 54/400 [04:10<29:41,  5.15s/it, lr=1e-5, step_loss=0.997]Steps:  14%|█▍        | 55/400 [04:15<29:36,  5.15s/it, lr=1e-5, step_loss=0.997]Steps:  14%|█▍        | 55/400 [04:15<29:36,  5.15s/it, lr=1e-5, step_loss=0.848]Steps:  14%|█▍        | 56/400 [04:21<31:42,  5.53s/it, lr=1e-5, step_loss=0.848]Steps:  14%|█▍        | 56/400 [04:21<31:42,  5.53s/it, lr=1e-5, step_loss=0.825]Steps:  14%|█▍        | 57/400 [04:26<30:36,  5.36s/it, lr=1e-5, step_loss=0.825]Steps:  14%|█▍        | 57/400 [04:26<30:36,  5.36s/it, lr=1e-5, step_loss=1.11] Steps:  14%|█▍        | 58/400 [04:31<29:55,  5.25s/it, lr=1e-5, step_loss=1.11]Steps:  14%|█▍        | 58/400 [04:31<29:55,  5.25s/it, lr=1e-5, step_loss=0.959]Steps:  15%|█▍        | 59/400 [04:36<29:41,  5.22s/it, lr=1e-5, step_loss=0.959]Steps:  15%|█▍        | 59/400 [04:36<29:41,  5.22s/it, lr=1e-5, step_loss=0.836]Steps:  15%|█▌        | 60/400 [04:42<29:37,  5.23s/it, lr=1e-5, step_loss=0.836]Steps:  15%|█▌        | 60/400 [04:42<29:37,  5.23s/it, lr=1e-5, step_loss=0.64] Steps:  15%|█▌        | 61/400 [04:48<31:16,  5.54s/it, lr=1e-5, step_loss=0.64]Steps:  15%|█▌        | 61/400 [04:48<31:16,  5.54s/it, lr=1e-5, step_loss=0.559]Steps:  16%|█▌        | 62/400 [04:53<30:31,  5.42s/it, lr=1e-5, step_loss=0.559]Steps:  16%|█▌        | 62/400 [04:53<30:31,  5.42s/it, lr=1e-5, step_loss=0.86] Steps:  16%|█▌        | 63/400 [04:58<29:48,  5.31s/it, lr=1e-5, step_loss=0.86]Steps:  16%|█▌        | 63/400 [04:58<29:48,  5.31s/it, lr=1e-5, step_loss=0.429]Steps:  16%|█▌        | 64/400 [05:03<29:12,  5.21s/it, lr=1e-5, step_loss=0.429]Steps:  16%|█▌        | 64/400 [05:03<29:12,  5.21s/it, lr=1e-5, step_loss=0.604]Steps:  16%|█▋        | 65/400 [05:08<29:14,  5.24s/it, lr=1e-5, step_loss=0.604]Steps:  16%|█▋        | 65/400 [05:08<29:14,  5.24s/it, lr=1e-5, step_loss=0.907]Steps:  16%|█▋        | 66/400 [05:15<30:54,  5.55s/it, lr=1e-5, step_loss=0.907]Steps:  16%|█▋        | 66/400 [05:15<30:54,  5.55s/it, lr=1e-5, step_loss=0.571]Steps:  17%|█▋        | 67/400 [05:20<29:54,  5.39s/it, lr=1e-5, step_loss=0.571]Steps:  17%|█▋        | 67/400 [05:20<29:54,  5.39s/it, lr=1e-5, step_loss=0.809]Steps:  17%|█▋        | 68/400 [05:25<29:22,  5.31s/it, lr=1e-5, step_loss=0.809]Steps:  17%|█▋        | 68/400 [05:25<29:22,  5.31s/it, lr=1e-5, step_loss=0.677]Steps:  17%|█▋        | 69/400 [05:30<28:41,  5.20s/it, lr=1e-5, step_loss=0.677]Steps:  17%|█▋        | 69/400 [05:30<28:41,  5.20s/it, lr=1e-5, step_loss=0.449]Steps:  18%|█▊        | 70/400 [05:35<28:23,  5.16s/it, lr=1e-5, step_loss=0.449]Steps:  18%|█▊        | 70/400 [05:35<28:23,  5.16s/it, lr=1e-5, step_loss=0.795]Steps:  18%|█▊        | 71/400 [05:41<30:29,  5.56s/it, lr=1e-5, step_loss=0.795]Steps:  18%|█▊        | 71/400 [05:41<30:29,  5.56s/it, lr=1e-5, step_loss=0.856]Steps:  18%|█▊        | 72/400 [05:46<29:26,  5.39s/it, lr=1e-5, step_loss=0.856]Steps:  18%|█▊        | 72/400 [05:46<29:26,  5.39s/it, lr=1e-5, step_loss=0.918]Steps:  18%|█▊        | 73/400 [05:51<28:37,  5.25s/it, lr=1e-5, step_loss=0.918]Steps:  18%|█▊[2024-07-03 07:14:23,087] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
        | 73/400 [05:51<28:37,  5.25s/it, lr=1e-5, step_loss=0.564]Steps:  18%|█▊        | 74/400 [05:56<28:14,  5.20s/it, lr=1e-5, step_loss=0.564]Steps:  18%|█▊        | 74/400 [05:56<28:14,  5.20s/it, lr=1e-5, step_loss=0.345]Steps:  19%|█▉        | 75/400 [05:59<24:13,  4.47s/it, lr=1e-5, step_loss=0.345]Steps:  19%|█▉        | 75/400 [05:59<24:13,  4.47s/it, lr=1e-5, step_loss=0.93] Steps:  19%|█▉        | 76/400 [06:05<26:59,  5.00s/it, lr=1e-5, step_loss=0.93]Steps:  19%|█▉        | 76/400 [06:05<26:59,  5.00s/it, lr=1e-5, step_loss=0.719]Steps:  19%|█▉        | 77/400 [06:11<27:07,  5.04s/it, lr=1e-5, step_loss=0.719]Steps:  19%|█▉        | 77/400 [06:11<27:07,  5.04s/it, lr=1e-5, step_loss=1.02] Steps:  20%|█▉        | 78/400 [06:15<26:55,  5.02s/it, lr=1e-5, step_loss=1.02]Steps:  20%|█▉        | 78/400 [06:15<26:55,  5.02s/it, lr=1e-5, step_loss=0.473]Steps:  20%|█▉        | 79/400 [06:19<24:55,  4.66s/it, lr=1e-5, step_loss=0.473]Steps:  20%|█▉        | 79/400 [06:19<24:55,  4.66s/it, lr=1e-5, step_loss=0.919]Steps:  20%|██        | 80/400 [06:24<25:34,  4.80s/it, lr=1e-5, step_loss=0.919]Steps:  20%|██        | 80/400 [06:24<25:34,  4.80s/it, lr=1e-5, step_loss=0.421]Steps:  20%|██        | 81/400 [06:31<28:05,  5.28s/it, lr=1e-5, step_loss=0.421]Steps:  20%|██        | 81/400 [06:31<28:05,  5.28s/it, lr=1e-5, step_loss=0.56] Steps:  20%|██        | 82/400 [06:36<27:31,  5.19s/it, lr=1e-5, step_loss=0.56]Steps:  20%|██        | 82/400 [06:36<27:31,  5.19s/it, lr=1e-5, step_loss=0.5] Steps:  21%|██        | 83/400 [06:41<27:03,  5.12s/it, lr=1e-5, step_loss=0.5]Steps:  21%|██        | 83/400 [06:41<27:03,  5.12s/it, lr=1e-5, step_loss=0.367]Steps:  21%|██        | 84/400 [06:46<26:57,  5.12s/it, lr=1e-5, step_loss=0.367]Steps:  21%|██        | 84/400 [06:46<26:57,  5.12s/it, lr=1e-5, step_loss=0.475]Steps:  21%|██▏       | 85/400 [06:51<26:53,  5.12s/it, lr=1e-5, step_loss=0.475]Steps:  21%|██▏       | 85/400 [06:51<26:53,  5.12s/it, lr=1e-5, step_loss=0.782]Steps:  22%|██▏       | 86/400 [06:57<28:26,  5.44s/it, lr=1e-5, step_loss=0.782]Steps:  22%|██▏       | 86/400 [06:57<28:26,  5.44s/it, lr=1e-5, step_loss=0.753]Steps:  22%|██▏       | 87/400 [07:02<27:55,  5.35s/it, lr=1e-5, step_loss=0.753]Steps:  22%|██▏       | 87/400 [07:02<27:55,  5.35s/it, lr=1e-5, step_loss=0.438]Steps:  22%|██▏       | 88/400 [07:07<27:15,  5.24s/it, lr=1e-5, step_loss=0.438]Steps:  22%|██▏       | 88/400 [07:07<27:15,  5.24s/it, lr=1e-5, step_loss=0.43] Steps:  22%|██▏       | 89/400 [07:12<26:46,  5.17s/it, lr=1e-5, step_loss=0.43]Steps:  22%|██▏       | 89/400 [07:12<26:46,  5.17s/it, lr=1e-5, step_loss=0.414]Steps:  22%|██▎       | 90/400 [07:16<24:45,  4.79s/it, lr=1e-5, step_loss=0.414]Steps:  22%|██▎       | 90/400 [07:16<24:45,  4.79s/it, lr=1e-5, step_loss=0.911]Steps:  23%|██▎       | 91/400 [07:23<26:57,  5.23s/it, lr=1e-5, step_loss=0.911]Steps:  23%|██▎       | 91/400 [07:23<26:57,  5.23s/it, lr=1e-5, step_loss=0.187]Steps:  23%|██▎       | 92/400 [07:27<26:26,  5.15s/it, lr=1e-5, step_loss=0.187]Steps:  23%|██▎       | 92/400 [07:27<26:26,  5.15s/it, lr=1e-5, step_loss=0.468]Steps:  23%|██▎       | 93/400 [07:32<26:00,  5.08s/it, lr=1e-5, step_loss=0.468]Steps:  23%|██▎       | 93/400 [07:32<26:00,  5.08s/it, lr=1e-5, step_loss=0.521]Steps:  24%|██▎       | 94/400 [07:38<26:00,  5.10s/it, lr=1e-5, step_loss=0.521]Steps:  24%|██▎       | 94/400 [07:38<26:00,  5.10s/it, lr=1e-5, step_loss=0.479]Steps:  24%|██▍       | 95/400 [07:43<25:53,  5.09s/it, lr=1e-5, step_loss=0.479]Steps:  24%|██▍       | 95/400 [07:43<25:53,  5.09s/it, lr=1e-5, step_loss=0.387]Steps:  24%|██▍       | 96/400 [07:48<25:47,  5.09s/it, lr=1e-5, step_loss=0.387]Steps:  24%|██▍       | 96/400 [07:48<25:47,  5.09s/it, lr=1e-5, step_loss=0.345]Steps:  24%|██▍       | 97/400 [07:53<25:33,  5.06s/it, lr=1e-5, step_loss=0.345]Steps:  24%|██▍       | 97/400 [07:53<25:33,  5.06s/it, lr=1e-5, step_loss=0.744]Steps:  24%|██▍       | 98/400 [07:58<25:40,  5.10s/it, lr=1e-5, step_loss=0.744]Steps:  24%|██▍       | 98/400 [07:58<25:40,  5.10s/it, lr=1e-5, step_loss=0.168]Steps:  25%|██▍       | 99/400 [08:03<25:25,  5.07s/it, lr=1e-5, step_loss=0.168]Steps:  25%|██▍       | 99/400 [08:03<25:25,  5.07s/it, lr=1e-5, step_loss=0.41] Steps:  25%|██▌       | 100/400 [08:08<25:27,  5.09s/it, lr=1e-5, step_loss=0.41]Steps:  25%|██▌       | 100/400 [08:08<25:27,  5.09s/it, lr=1e-5, step_loss=0.406]Steps:  25%|██▌       | 101/400 [08:14<27:18,  5.48s/it, lr=1e-5, step_loss=0.406]Steps:  25%|██▌       | 101/400 [08:14<27:18,  5.48s/it, lr=1e-5, step_loss=0.429]Steps:  26%|██▌       | 102/400 [08:19<26:27,  5.33s/it, lr=1e-5, step_loss=0.429]Steps:  26%|██▌       | 102/400 [08:19<26:27,  5.33s/it, lr=1e-5, step_loss=0.189]Steps:  26%|██▌       | 103/400 [08:24<25:49,  5.22s/it, lr=1e-5, step_loss=0.189]Steps:  26%|██▌       | 103/400 [08:24<25:49,  5.22s/it, lr=1e-5, step_loss=0.683]Steps:  26%|██▌       | 104/400 [08:30<25:39,  5.20s/it, lr=1e-5, step_loss=0.683]Steps:  26%|██▌       | 104/400 [08:30<25:39,  5.20s/it, lr=1e-5, step_loss=0.626]Steps:  26%|██▋       | 105/400 [08:35<25:28,  5.18s/it, lr=1e-5, step_loss=0.626]Steps:  26%|██▋       | 105/400 [08:35<25:28,  5.18s/it, lr=1e-5, step_loss=0.32] Steps:  26%|██▋       | 106/400 [09:07<1:05:57, 13.46s/it, lr=1e-5, step_loss=0.32]Steps:  26%|██▋       | 106/400 [09:07<1:05:57, 13.46s/it, lr=1e-5, step_loss=0.427]Steps:  27%|██▋       | 107/400 [09:13<53:34, 10.97s/it, lr=1e-5, step_loss=0.427]  Steps:  27%|██▋       | 107/400 [09:13<53:34, 10.97s/it, lr=1e-5, step_loss=0.383]Steps:  27%|██▋       | 108/400 [09:18<44:40,  9.18s/it, lr=1e-5, step_loss=0.383]Steps:  27%|██▋       | 108/400 [09:18<44:40,  9.18s/it, lr=1e-5, step_loss=0.182]Steps:  27%|██▋       | 109/400 [09:23<38:57,  8.03s/it, lr=1e-5, step_loss=0.182]Steps:  27%|██▋       | 109/400 [09:23<38:57,  8.03s/it, lr=1e-5, step_loss=0.234]Steps:  28%|██▊       | 110/400 [09:28<34:54,  7.22s/it, lr=1e-5, step_loss=0.234]Steps:  28%|██▊       | 110/400 [09:28<34:54,  7.22s/it, lr=1e-5, step_loss=0.312]Steps:  28%|██▊       | 111/400 [09:35<33:40,  6.99s/it, lr=1e-5, step_loss=0.312]Steps:  28%|██▊       | 111/400 [09:36<33:40,  6.99s/it, lr=1e-5, step_loss=0.153]Steps:  28%|██▊       | 112/400 [09:41<32:06,  6.69s/it, lr=1e-5, step_loss=0.153]Steps:  28%|██▊       | 112/400 [09:41<32:06,  6.69s/it, lr=1e-5, step_loss=0.112]Steps:  28%|██▊       | 113/400 [09:46<29:45,  6.22s/it, lr=1e-5, step_loss=0.112]Steps:  28%|██▊       | 113/400 [09:47<29:45,  6.22s/it, lr=1e-5, step_loss=0.421]Steps:  28%|██▊       | 114/400 [09:52<29:38,  6.22s/it, lr=1e-5, step_loss=0.421]Steps:  28%|██▊       | 114/400 [09:52<29:38,  6.22s/it, lr=1e-5, step_loss=0.567]Steps:  29%|██▉       | 115/400 [09:57<27:58,  5.89s/it, lr=1e-5, step_loss=0.567]Steps:  29%|██▉       | 115/400 [09:58<27:58,  5.89s/it, lr=1e-5, step_loss=0.63] Steps:  29%|██▉       | 116/400 [10:04<29:30,  6.24s/it, lr=1e-5, step_loss=0.63]Steps:  29%|██▉       | 116/400 [10:04<29:30,  6.24s/it, lr=1e-5, step_loss=0.701]Steps:  29%|██▉       | 117/400 [10:09<27:34,  5.85s/it, lr=1e-5, step_loss=0.701]Steps:  29%|██▉       | 117/400 [10:09<27:34,  5.85s/it, lr=1e-5, step_loss=0.188]Steps:  30%|██▉       | 118/400 [10:14<26:10,  5.57s/it, lr=1e-5, step_loss=0.188]Steps:  30%|██▉       | 118/400 [10:14<26:10,  5.57s/it, lr=1e-5, step_loss=0.453]Steps:  30%|██▉       | 119/400 [10:18<23:35,  5.04s/it, lr=1e-5, step_loss=0.453]Steps:  30%|██▉       | 119/400 [10:18<23:35,  5.04s/it, lr=1e-5, step_loss=0.539]Steps:  30%|███       | 120/400 [10:23<23:46,  5.09s/it, lr=1e-5, step_loss=0.539]Steps:  30%|███       | 120/400 [10:23<23:46,  5.09s/it, lr=1e-5, step_loss=0.118]Steps:  30%|███       | 121/400 [10:29<25:24,  5.46s/it, lr=1e-5, step_loss=0.118]Steps:  30%|███       | 121/400 [10:29<25:24,  5.46s/it, lr=1e-5, step_loss=0.263]Steps:  30%|███       | 122/400 [10:34<24:34,  5.30s/it, lr=1e-5, step_loss=0.263]Steps:  30%|███       | 122/400 [10:34<24:34,  5.30s/it, lr=1e-5, step_loss=0.18] Steps:  31%|███       | 123/400 [10:39<24:15,  5.25s/it, lr=1e-5, step_loss=0.18]Steps:  31%|███       | 123/400 [10:39<24:15,  5.25s/it, lr=1e-5, step_loss=0.405]Steps:  31%|███       | 124/400 [10:44<23:47,  5.17s/it, lr=1e-5, step_loss=0.405]Steps:  31%|███       | 124/400 [10:44<23:47,  5.17s/it, lr=1e-5, step_loss=0.34] Steps:  31%|███▏      | 125/400 [10:50<23:37,  5.16s/it, lr=1e-5, step_loss=0.34]Steps:  31%|███▏      | 125/400 [10:50<23:37,  5.16s/it, lr=1e-5, step_loss=0.364]Steps:  32%|███▏      | 126/400 [10:55<23:27,  5.14s/it, lr=1e-5, step_loss=0.364]Steps:  32%|███▏      | 126/400 [10:55<23:27,  5.14s/it, lr=1e-5, step_loss=0.617]Steps:  32%|███▏      | 127/400 [11:00<23:22,  5.14s/it, lr=1e-5, step_loss=0.617]Steps:  32%|███▏      | 127/400 [11:00<23:22,  5.14s/it, lr=1e-5, step_loss=0.268]Steps:  32%|███▏      | 128/400 [11:05<23:02,  5.08s/it, lr=1e-5, step_loss=0.268]Steps:  32%|███▏      | 128/400 [11:05<23:02,  5.08s/it, lr=1e-5, step_loss=0.29] Steps:  32%|███▏      | 129/400 [11:10<22:47,  5.05s/it, lr=1e-5, step_loss=0.29]Steps:  32%|███▏      | 129/400 [11:10<22:47,  5.05s/it, lr=1e-5, step_loss=0.348]Steps:  32%|███▎      | 130/400 [11:15<23:03,  5.13s/it, lr=1e-5, step_loss=0.348]Steps:  32%|███▎      | 130/400 [11:15<23:03,  5.13s/it, lr=1e-5, step_loss=0.437]Steps:  33%|███▎      | 131/400 [11:21<24:38,  5.50s/it, lr=1e-5, step_loss=0.437]Steps:  33%|███▎      | 131/400 [11:21<24:38,  5.50s/it, lr=1e-5, step_loss=0.115]Steps:  33%|███▎      | 132/400 [11:26<23:52,  5.34s/it, lr=1e-5, step_loss=0.115]Steps:  33%|███▎      | 132/400 [11:26<23:52,  5.34s/it, lr=1e-5, step_loss=0.259]Steps:  33%|███▎      | 133/400 [11:32<23:29,  5.28s/it, lr=1e-5, step_loss=0.259]Steps:  33%|███▎      | 133/400 [11:32<23:29,  5.28s/it, lr=1e-5, step_loss=0.13] Steps:  34%|███▎      | 134/400 [11:37<23:00,  5.19s/it, lr=1e-5, step_loss=0.13]Steps:  34%|███▎      | 134/400 [11:37<23:00,  5.19s/it, lr=1e-5, step_loss=0.345]Steps:  34%|███▍      | 135/400 [11:42<22:50,  5.17s/it, lr=1e-5, step_loss=0.345]Steps:  34%|███▍      | 135/400 [11:42<22:50,  5.17s/it, lr=1e-5, step_loss=0.161]Steps:  34%|███▍      | 136/400 [11:48<24:28,  5.56s/it, lr=1e-5, step_loss=0.161]Steps:  34%|███▍      | 136/400 [11:48<24:28,  5.56s/it, lr=1e-5, step_loss=0.471]Steps:  34%|███▍      | 137/400 [11:53<23:38,  5.39s/it, lr=1e-5, step_loss=0.471]Steps:  34%|███▍      | 137/400 [11:53<23:38,  5.39s/it, lr=1e-5, step_loss=0.0934]Steps:  34%|███▍      | 138/400 [11:58<22:59,  5.27s/it, lr=1e-5, step_loss=0.0934]Steps:  34%|███▍      | 138/400 [11:58<22:59,  5.27s/it, lr=1e-5, step_loss=0.186] Steps:  35%|███▍      | 139/400 [12:02<20:56,  4.81s/it, lr=1e-5, step_loss=0.186]Steps:  35%|███▍      | 139/400 [12:02<20:56,  4.81s/it, lr=1e-5, step_loss=0.355]Steps:  35%|███▌      | 140/400 [12:07<21:32,  4.97s/it, lr=1e-5, step_loss=0.355]Steps:  35%|███▌      | 140/400 [12:07<21:32,  4.97s/it, lr=1e-5, step_loss=0.229]Steps:  35%|███▌      | 141/400 [12:13<22:59,  5.33s/it, lr=1e-5, step_loss=0.229]Steps:  35%|███▌      | 141/400 [12:13<22:59,  5.33s/it, lr=1e-5, step_loss=0.133]Steps:  36%|███▌      | 142/400 [12:18<22:27,  5.22s/it, lr=1e-5, step_loss=0.133]Steps:  36%|███▌      | 142/400 [12:18<22:27,  5.22s/it, lr=1e-5, step_loss=0.159]Steps:  36%|███▌      | 143/400 [12:23<22:16,  5.20s/it, lr=1e-5, step_loss=0.159]Steps:  36%|███▌      | 143/400 [12:23<22:16,  5.20s/it, lr=1e-5, step_loss=0.34] Steps:  36%|███▌      | 144/400 [12:28<21:53,  5.13s/it, lr=1e-5, step_loss=0.34]Steps:  36%|███▌      | 144/400 [12:28<21:53,  5.13s/it, lr=1e-5, step_loss=0.306]Steps:  36%|███▋      | 145/400 [12:34<21:48,  5.13s/it, lr=1e-5, step_loss=0.306]Steps:  36%|███▋      | 145/400 [12:34<21:48,  5.13s/it, lr=1e-5, step_loss=0.266]Steps:  36%|███▋      | 146/400 [12:40<23:23,  5.53s/it, lr=1e-5, step_loss=0.266]Steps:  36%|███▋      | 146/400 [12:40<23:23,  5.53s/it, lr=1e-5, step_loss=0.22] Steps:  37%|███▋      | 147/400 [12:45<22:38,  5.37s/it, lr=1e-5, step_loss=0.22]Steps:  37%|███▋      | 147/400 [12:45<22:38,  5.37s/it, lr=1e-5, step_loss=0.417]Steps:  37%|███▋      | 148/400 [12:50<22:03,  5.25s/it, lr=1e-5, step_loss=0.417]Steps:  37%|███▋      | 148/400 [12:50<22:03,  5.25s/it, lr=1e-5, step_loss=0.157]Steps:  37%|███▋      | 149/400 [12:55<21:49,  5.22s/it, lr=1e-5, step_loss=0.157]Steps:  37%|███▋      | 149/400 [12:55<21:49,  5.22s/it, lr=1e-5, step_loss=0.422]Steps:  38%|███▊      | 150/400 [13:00<21:33,  5.18s/it, lr=1e-5, step_loss=0.422]Steps:  38%|███▊      | 150/400 [13:00<21:33,  5.18s/it, lr=1e-5, step_loss=0.247]Steps:  38%|███▊      | 151/400 [13:05<21:20,  5.14s/it, lr=1e-5, step_loss=0.247]Steps:  38%|███▊      | 151/400 [13:05<21:20,  5.14s/it, lr=1e-5, step_loss=0.513]Steps:  38%|███▊      | 152/400 [13:10<21:04,  5.10s/it, lr=1e-5, step_loss=0.513]Steps:  38%|███▊      | 152/400 [13:10<21:04,  5.10s/it, lr=1e-5, step_loss=0.461]Steps:  38%|███▊      | 153/400 [13:15<21:03,  5.12s/it, lr=1e-5, step_loss=0.461]Steps:  38%|███▊      | 153/400 [13:15<21:03,  5.12s/it, lr=1e-5, step_loss=0.467]Steps:  38%|███▊      | 154/400 [13:20<20:46,  5.07s/it, lr=1e-5, step_loss=0.467]Steps:  38%|███▊      | 154/400 [13:20<20:46,  5.07s/it, lr=1e-5, step_loss=0.375]Steps:  39%|███▉      | 155/400 [13:24<19:18,  4.73s/it, lr=1e-5, step_loss=0.375]Steps:  39%|███▉      | 155/400 [13:24<19:18,  4.73s/it, lr=1e-5, step_loss=0.359]Steps:  39%|███▉      | 156/400 [13:30<20:59,  5.16s/it, lr=1e-5, step_loss=0.359]Steps:  39%|███▉      | 156/400 [13:31<20:59,  5.16s/it, lr=1e-5, step_loss=0.305]Steps:  39%|███▉      | 157/400 [13:36<20:54,  5.16s/it, lr=1e-5, step_loss=0.305]Steps:  39%|███▉      | 157/400 [13:36<20:54,  5.16s/it, lr=1e-5, step_loss=0.25] Steps:  40%|███▉      | 158/400 [13:41<20:35,  5.10s/it, lr=1e-5, step_loss=0.25]Steps:  40%|███▉      | 158/400 [13:41<20:35,  5.10s/it, lr=1e-5, step_loss=0.293]Steps:  40%|███▉      | 159/400 [13:46<20:19,  5.06s/it, lr=1e-5, step_loss=0.293]Steps:  40%|███▉      | 159/400 [13:46<20:19,  5.06s/it, lr=1e-5, step_loss=0.235]Steps:  40%|████      | 160/400 [13:51<20:30,  5.13s/it, lr=1e-5, step_loss=0.235]Steps:  40%|████      | 160/400 [13:51<20:30,  5.13s/it, lr=1e-5, step_loss=0.217]Steps:  40%|████      | 161/400 [13:57<21:34,  5.42s/it, lr=1e-5, step_loss=0.217]Steps:  40%|████      | 161/400 [13:57<21:34,  5.42s/it, lr=1e-5, step_loss=0.256]Steps:  40%|████      | 162/400 [14:02<20:56,  5.28s/it, lr=1e-5, step_loss=0.256]Steps:  40%|████      | 162/400 [14:02<20:56,  5.28s/it, lr=1e-5, step_loss=0.229]Steps:  41%|████      | 163/400 [14:07<20:42,  5.24s/it, lr=1e-5, step_loss=0.229]Steps:  41%|████      | 163/400 [14:07<20:42,  5.24s/it, lr=1e-5, step_loss=0.087]Steps:  41%|████      | 164/400 [14:12<20:20,  5.17s/it, lr=1e-5, step_loss=0.087]Steps:  41%|████      | 164/400 [14:12<20:20,  5.17s/it, lr=1e-5, step_loss=0.0801]Steps:  41%|████▏     | 165/400 [14:17<20:12,  5.16s/it, lr=1e-5, step_loss=0.0801]Steps:  41%|████▏     | 165/400 [14:17<20:12,  5.16s/it, lr=1e-5, step_loss=0.21]  Steps:  42%|████▏     | 166/400 [14:24<21:30,  5.52s/it, lr=1e-5, step_loss=0.21]Steps:  42%|████▏     | 166/400 [14:24<21:30,  5.52s/it, lr=1e-5, step_loss=0.151]Steps:  42%|████▏     | 167/400 [14:29<20:46,  5.35s/it, lr=1e-5, step_loss=0.151]Steps:  42%|████▏     | 167/400 [14:29<20:46,  5.35s/it, lr=1e-5, step_loss=0.295]Steps:  42%|████▏     | 168/400 [14:34<20:15,  5.24s/it, lr=1e-5, step_loss=0.295]Steps:  42%|████▏     | 168/400 [14:34<20:15,  5.24s/it, lr=1e-5, step_loss=0.197]Steps:  42%|████▏     | 169/400 [14:39<20:03,  5.21s/it, lr=1e-5, step_loss=0.197]Steps:  42%|████▏     | 169/400 [14:39<20:03,  5.21s/it, lr=1e-5, step_loss=0.209]Steps:  42%|████▎     | 170/400 [14:44<19:51,  5.18s/it, lr=1e-5, step_loss=0.209]Steps:  42%|████▎     | 170/400 [14:44<19:51,  5.18s/it, lr=1e-5, step_loss=0.134]Steps:  43%|████▎     | 171/400 [14:50<21:09,  5.54s/it, lr=1e-5, step_loss=0.134]Steps:  43%|████▎     | 171/400 [14:50<21:09,  5.54s/it, lr=1e-5, step_loss=0.167]Steps:  43%|████▎     | 172/400 [14:55<20:37,  5.43s/it, lr=1e-5, step_loss=0.167]Steps:  43%|████▎     | 172/400 [14:55<20:37,  5.43s/it, lr=1e-5, step_loss=0.333]Steps:  43%|████▎     | 173/400 [15:00<20:00,  5.29s/it, lr=1e-5, step_loss=0.333]Steps:  43%|████▎     | 173/400 [15:00<20:00,  5.29s/it, lr=1e-5, step_loss=0.108]Steps:  44%|████▎     | 174/400 [15:05<19:31,  5.18s/it, lr=1e-5, step_loss=0.108]Steps:  44%|████▎     | 174/400 [15:05<19:31,  5.18s/it, lr=1e-5, step_loss=0.281]Steps:  44%|████▍     | 175/400 [15:10<19:33,  5.21s/it, lr=1e-5, step_loss=0.281]Steps:  44%|████▍     | 175/400 [15:10<19:33,  5.21s/it, lr=1e-5, step_loss=0.262]Steps:  44%|████▍     | 176/400 [15:17<20:31,  5.50s/it, lr=1e-5, step_loss=0.262]Steps:  44%|████▍     | 176/400 [15:17<20:31,  5.50s/it, lr=1e-5, step_loss=0.113]Steps:  44%|████▍     | 177/400 [15:22<19:52,  5.35s/it, lr=1e-5, step_loss=0.113]Steps:  44%|████▍     | 177/400 [15:22<19:52,  5.35s/it, lr=1e-5, step_loss=0.133]Steps:  44%|████▍     | 178/400 [15:27<19:36,  5.30s/it, lr=1e-5, step_loss=0.133]Steps:  44%|████▍     | 178/400 [15:27<19:36,  5.30s/it, lr=1e-5, step_loss=0.136]Steps:  45%|████▍     | 179/400 [15:32<19:05,  5.18s/it, lr=1e-5, step_loss=0.136]Steps:  45%|████▍     | 179/400 [15:32<19:05,  5.18s/it, lr=1e-5, step_loss=0.123]Steps:  45%|████▌     | 180/400 [15:36<17:40,  4.82s/it, lr=1e-5, step_loss=0.123]Steps:  45%|████▌     | 180/400 [15:36<17:40,  4.82s/it, lr=1e-5, step_loss=0.164]Steps:  45%|████▌     | 181/400 [15:42<18:58,  5.20s/it, lr=1e-5, step_loss=0.164]Steps:  45%|████▌     | 181/400 [15:42<18:58,  5.20s/it, lr=1e-5, step_loss=0.213]Steps:  46%|████▌     | 182/400 [15:46<17:23,  4.79s/it, lr=1e-5, step_loss=0.213]Steps:  46%|████▌     | 182/400 [15:46<17:23,  4.79s/it, lr=1e-5, step_loss=0.329]Steps:  46%|████▌     | 183/400 [15:51<17:38,  4.88s/it, lr=1e-5, step_loss=0.329]Steps:  46%|████▌     | 183/400 [15:51<17:38,  4.88s/it, lr=1e-5, step_loss=0.308]Steps:  46%|████▌     | 184/400 [15:56<17:36,  4.89s/it, lr=1e-5, step_loss=0.308]Steps:  46%|████▌     | 184/400 [15:56<17:36,  4.89s/it, lr=1e-5, step_loss=0.195]Steps:  46%|████▋     | 185/400 [16:01<17:43,  4.94s/it, lr=1e-5, step_loss=0.195]Steps:  46%|████▋     | 185/400 [16:01<17:43,  4.94s/it, lr=1e-5, step_loss=0.218]Steps:  46%|████▋     | 186/400 [16:07<19:07,  5.36s/it, lr=1e-5, step_loss=0.218]Steps:  46%|████▋     | 186/400 [16:07<19:07,  5.36s/it, lr=1e-5, step_loss=0.229]Steps:  47%|████▋     | 187/400 [16:12<18:36,  5.24s/it, lr=1e-5, step_loss=0.229]Steps:  47%|████▋     | 187/400 [16:12<18:36,  5.24s/it, lr=1e-5, step_loss=0.218]Steps:  47%|████▋     | 188/400 [16:17<18:11,  5.15s/it, lr=1e-5, step_loss=0.218]Steps:  47%|████▋     | 188/400 [16:17<18:11,  5.15s/it, lr=1e-5, step_loss=0.194]Steps:  47%|████▋     | 189/400 [16:22<18:07,  5.15s/it, lr=1e-5, step_loss=0.194]Steps:  47%|████▋     | 189/400 [16:22<18:07,  5.15s/it, lr=1e-5, step_loss=0.158]Steps:  48%|████▊     | 190/400 [16:27<17:58,  5.13s/it, lr=1e-5, step_loss=0.158]Steps:  48%|████▊     | 190/400 [16:27<17:58,  5.13s/it, lr=1e-5, step_loss=0.108]Steps:  48%|████▊     | 191/400 [16:33<19:02,  5.47s/it, lr=1e-5, step_loss=0.108]Steps:  48%|████▊     | 191/400 [16:33<19:02,  5.47s/it, lr=1e-5, step_loss=0.165]Steps:  48%|████▊     | 192/400 [16:39<18:36,  5.37s/it, lr=1e-5, step_loss=0.165]Steps:  48%|████▊     | 192/400 [16:39<18:36,  5.37s/it, lr=1e-5, step_loss=0.225]Steps:  48%|████▊     | 193/400 [16:44<18:07,  5.26s/it, lr=1e-5, step_loss=0.225]Steps:  48%|████▊     | 193/400 [16:44<18:07,  5.26s/it, lr=1e-5, step_loss=0.332]Steps:  48%|████▊     | 194/400 [16:48<17:42,  5.16s/it, lr=1e-5, step_loss=0.332]Steps:  48%|████▊     | 194/400 [16:48<17:42,  5.16s/it, lr=1e-5, step_loss=0.0836]Steps:  49%|████▉     | 195/400 [16:54<17:46,  5.20s/it, lr=1e-5, step_loss=0.0836]Steps:  49%|████▉     | 195/400 [16:54<17:46,  5.20s/it, lr=1e-5, step_loss=0.117] Steps:  49%|████▉     | 196/400 [17:00<18:41,  5.50s/it, lr=1e-5, step_loss=0.117]Steps:  49%|████▉     | 196/400 [17:00<18:41,  5.50s/it, lr=1e-5, step_loss=0.161]Steps:  49%|████▉     | 197/400 [17:05<18:00,  5.32s/it, lr=1e-5, step_loss=0.161]Steps:  49%|████▉     | 197/400 [17:05<18:00,  5.32s/it, lr=1e-5, step_loss=0.232]Steps:  50%|████▉     | 198/400 [17:10<17:45,  5.28s/it, lr=1e-5, step_loss=0.232]Steps:  50%|████▉     | 198/400 [17:10<17:45,  5.28s/it, lr=1e-5, step_loss=0.152]Steps:  50%|████▉     | 199/400 [17:15<17:21,  5.18s/it, lr=1e-5, step_loss=0.152]Steps:  50%|████▉     | 199/400 [17:15<17:21,  5.18s/it, lr=1e-5, step_loss=0.176]Steps:  50%|█████     | 200/400 [17:20<17:10,  5.15s/it, lr=1e-5, step_loss=0.176]Steps:  50%|█████     | 200/400 [17:20<17:10,  5.15s/it, lr=1e-5, step_loss=0.0921]Steps:  50%|█████     | 201/400 [17:27<18:22,  5.54s/it, lr=1e-5, step_loss=0.0921]Steps:  50%|█████     | 201/400 [17:27<18:22,  5.54s/it, lr=1e-5, step_loss=0.216] Steps:  50%|█████     | 202/400 [17:32<17:44,  5.37s/it, lr=1e-5, step_loss=0.216]Steps:  50%|█████     | 202/400 [17:32<17:44,  5.37s/it, lr=1e-5, step_loss=0.178]Steps:  51%|█████     | 203/400 [17:37<17:16,  5.26s/it, lr=1e-5, step_loss=0.178]Steps:  51%|█████     | 203/400 [17:37<17:16,  5.26s/it, lr=1e-5, step_loss=0.192]Steps:  51%|█████     | 204/400 [17:42<17:05,  5.23s/it, lr=1e-5, step_loss=0.192]Steps:  51%|█████     | 204/400 [17:42<17:05,  5.23s/it, lr=1e-5, step_loss=0.197]Steps:  51%|█████▏    | 205/400 [17:47<16:51,  5.19s/it, lr=1e-5, step_loss=0.197]Steps:  51%|█████▏    | 205/400 [17:47<16:51,  5.19s/it, lr=1e-5, step_loss=0.174]Steps:  52%|█████▏    | 206/400 [17:53<17:37,  5.45s/it, lr=1e-5, step_loss=0.174]Steps:  52%|█████▏    | 206/400 [17:53<17:37,  5.45s/it, lr=1e-5, step_loss=0.285]Steps:  52%|█████▏    | 207/400 [17:58<17:15,  5.36s/it, lr=1e-5, step_loss=0.285]Steps:  52%|█████▏    | 207/400 [17:58<17:15,  5.36s/it, lr=1e-5, step_loss=0.111]Steps:  52%|█████▏    | 208/400 [18:03<16:48,  5.25s/it, lr=1e-5, step_loss=0.111]Steps:  52%|█████▏    | 208/400 [18:03<16:48,  5.25s/it, lr=1e-5, step_loss=0.181]Steps:  52%|█████▏    | 209/400 [18:08<16:27,  5.17s/it, lr=1e-5, step_loss=0.181]Steps:  52%|█████▏    | 209/400 [18:08<16:27,  5.17s/it, lr=1e-5, step_loss=0.203]Steps:  52%|█████▎    | 210/400 [18:13<16:29,  5.21s/it, lr=1e-5, step_loss=0.203]Steps:  52%|█████▎    | 210/400 [18:13<16:29,  5.21s/it, lr=1e-5, step_loss=0.183]Steps:  53%|█████▎    | 211/400 [19:00<55:42, 17.68s/it, lr=1e-5, step_loss=0.183]Steps:  53%|█████▎    | 211/400 [19:00<55:42, 17.68s/it, lr=1e-5, step_loss=0.361]Steps:  53%|█████▎    | 212/400 [19:05<43:28, 13.87s/it, lr=1e-5, step_loss=0.361]Steps:  53%|█████▎    | 212/400 [19:05<43:28, 13.87s/it, lr=1e-5, step_loss=0.125]Steps:  53%|█████▎    | 213/400 [19:10<34:55, 11.21s/it, lr=1e-5, step_loss=0.125]Steps:  53%|█████▎    | 213/400 [19:10<34:55, 11.21s/it, lr=1e-5, step_loss=0.072]Steps:  54%|█████▎    | 214/400 [19:15<29:08,  9.40s/it, lr=1e-5, step_loss=0.072]Steps:  54%|█████▎    | 214/400 [19:15<29:08,  9.40s/it, lr=1e-5, step_loss=0.24] Steps:  54%|█████▍    | 215/400 [19:20<25:01,  8.12s/it, lr=1e-5, step_loss=0.24]Steps:  54%|█████▍    | 215/400 [19:20<25:01,  8.12s/it, lr=1e-5, step_loss=0.157]Steps:  54%|█████▍    | 216/400 [19:27<23:10,  7.56s/it, lr=1e-5, step_loss=0.157]Steps:  54%|█████▍    | 216/400 [19:27<23:10,  7.56s/it, lr=1e-5, step_loss=0.202]Steps:  54%|█████▍    | 217/400 [19:32<20:50,  6.83s/it, lr=1e-5, step_loss=0.202]Steps:  54%|█████▍    | 217/400 [19:32<20:50,  6.83s/it, lr=1e-5, step_loss=0.144]Steps:  55%|█████▍    | 218/400 [19:37<19:04,  6.29s/it, lr=1e-5, step_loss=0.144]Steps:  55%|█████▍    | 218/400 [19:37<19:04,  6.29s/it, lr=1e-5, step_loss=0.115]Steps:  55%|█████▍    | 219/400 [19:42<17:45,  5.89s/it, lr=1e-5, step_loss=0.115]Steps:  55%|█████▍    | 219/400 [19:42<17:45,  5.89s/it, lr=1e-5, step_loss=0.169]Steps:  55%|█████▌    | 220/400 [19:47<17:08,  5.71s/it, lr=1e-5, step_loss=0.169]Steps:  55%|█████▌    | 220/400 [19:47<17:08,  5.71s/it, lr=1e-5, step_loss=0.205]Steps:  55%|█████▌    | 221/400 [19:53<17:27,  5.85s/it, lr=1e-5, step_loss=0.205]Steps:  55%|█████▌    | 221/400 [19:53<17:27,  5.85s/it, lr=1e-5, step_loss=0.132]Steps:  56%|█████▌    | 222/400 [19:58<16:34,  5.59s/it, lr=1e-5, step_loss=0.132]Steps:  56%|█████▌    | 222/400 [19:58<16:34,  5.59s/it, lr=1e-5, step_loss=0.0991]Steps:  56%|█████▌    | 223/400 [20:03<16:05,  5.46s/it, lr=1e-5, step_loss=0.0991]Steps:  56%|█████▌    | 223/400 [20:03<16:05,  5.46s/it, lr=1e-5, step_loss=0.112] Steps:  56%|█████▌    | 224/400 [20:08<15:35,  5.32s/it, lr=1e-5, step_loss=0.112]Steps:  56%|█████▌    | 224/400 [20:08<15:35,  5.32s/it, lr=1e-5, step_loss=0.108]Steps:  56%|█████▋    | 225/400 [20:13<15:19,  5.25s/it, lr=1e-5, step_loss=0.108]Steps:  56%|█████▋    | 225/400 [20:13<15:19,  5.25s/it, lr=1e-5, step_loss=0.183]Steps:  56%|█████▋    | 226/400 [20:20<16:17,  5.62s/it, lr=1e-5, step_loss=0.183]Steps:  56%|█████▋    | 226/400 [20:20<16:17,  5.62s/it, lr=1e-5, step_loss=0.112]Steps:  57%|█████▋    | 227/400 [20:25<15:38,  5.42s/it, lr=1e-5, step_loss=0.112]Steps:  57%|█████▋    | 227/400 [20:25<15:38,  5.42s/it, lr=1e-5, step_loss=0.0977]Steps:  57%|█████▋    | 228/400 [20:30<15:09,  5.29s/it, lr=1e-5, step_loss=0.0977]Steps:  57%|█████▋    | 228/400 [20:30<15:09,  5.29s/it, lr=1e-5, step_loss=0.107] Steps:  57%|█████▋    | 229/400 [20:35<14:57,  5.25s/it, lr=1e-5, step_loss=0.107]Steps:  57%|█████▋    | 229/400 [20:35<14:57,  5.25s/it, lr=1e-5, step_loss=0.183]Steps:  57%|█████▊    | 230/400 [20:40<14:45,  5.21s/it, lr=1e-5, step_loss=0.183]Steps:  57%|█████▊    | 230/400 [20:40<14:45,  5.21s/it, lr=1e-5, step_loss=0.0719]Steps:  58%|█████▊    | 231/400 [20:46<15:26,  5.48s/it, lr=1e-5, step_loss=0.0719]Steps:  58%|█████▊    | 231/400 [20:46<15:26,  5.48s/it, lr=1e-5, step_loss=0.146] Steps:  58%|█████▊    | 232/400 [20:51<15:03,  5.38s/it, lr=1e-5, step_loss=0.146]Steps:  58%|█████▊    | 232/400 [20:51<15:03,  5.38s/it, lr=1e-5, step_loss=0.215]Steps:  58%|█████▊    | 233/400 [20:56<14:35,  5.24s/it, lr=1e-5, step_loss=0.215]Steps:  58%|█████▊    | 233/400 [20:56<14:35,  5.24s/it, lr=1e-5, step_loss=0.136]Steps:  58%|█████▊    | 234/400 [21:01<14:17,  5.16s/it, lr=1e-5, step_loss=0.136]Steps:  58%|█████▊    | 234/400 [21:01<14:17,  5.16s/it, lr=1e-5, step_loss=0.135]Steps:  59%|█████▉    | 235/400 [21:06<14:15,  5.18s/it, lr=1e-5, step_loss=0.135]Steps:  59%|█████▉    | 235/400 [21:06<14:15,  5.18s/it, lr=1e-5, step_loss=0.185]Steps:  59%|█████▉    | 236/400 [21:13<15:04,  5.51s/it, lr=1e-5, step_loss=0.185]Steps:  59%|█████▉    | 236/400 [21:13<15:04,  5.51s/it, lr=1e-5, step_loss=0.136]Steps:  59%|█████▉    | 237/400 [21:18<14:32,  5.35s/it, lr=1e-5, step_loss=0.136]Steps:  59%|█████▉    | 237/400 [21:18<14:32,  5.35s/it, lr=1e-5, step_loss=0.15] Steps:  60%|█████▉    | 238/400 [21:23<14:15,  5.28s/it, lr=1e-5, step_loss=0.15]Steps:  60%|█████▉    | 238/400 [21:23<14:15,  5.28s/it, lr=1e-5, step_loss=0.129]Steps:  60%|█████▉    | 239/400 [21:28<13:54,  5.18s/it, lr=1e-5, step_loss=0.129]Steps:  60%|█████▉    | 239/400 [21:28<13:54,  5.18s/it, lr=1e-5, step_loss=0.139]Steps:  60%|██████    | 240/400 [21:33<13:47,  5.17s/it, lr=1e-5, step_loss=0.139]Steps:  60%|██████    | 240/400 [21:33<13:47,  5.17s/it, lr=1e-5, step_loss=0.132]Steps:  60%|██████    | 241/400 [21:40<14:48,  5.59s/it, lr=1e-5, step_loss=0.132]Steps:  60%|██████    | 241/400 [21:40<14:48,  5.59s/it, lr=1e-5, step_loss=0.0778]Steps:  60%|██████    | 242/400 [21:44<14:13,  5.40s/it, lr=1e-5, step_loss=0.0778]Steps:  60%|██████    | 242/400 [21:45<14:13,  5.40s/it, lr=1e-5, step_loss=0.164] Steps:  61%|██████    | 243/400 [21:49<13:48,  5.28s/it, lr=1e-5, step_loss=0.164]Steps:  61%|██████    | 243/400 [21:49<13:48,  5.28s/it, lr=1e-5, step_loss=0.0911]Steps:  61%|██████    | 244/400 [21:55<13:34,  5.22s/it, lr=1e-5, step_loss=0.0911]Steps:  61%|██████    | 244/400 [21:55<13:34,  5.22s/it, lr=1e-5, step_loss=0.154] Steps:  61%|██████▏   | 245/400 [22:00<13:21,  5.17s/it, lr=1e-5, step_loss=0.154]Steps:  61%|██████▏   | 245/400 [22:00<13:21,  5.17s/it, lr=1e-5, step_loss=0.239]Steps:  62%|██████▏   | 246/400 [22:06<13:58,  5.45s/it, lr=1e-5, step_loss=0.239]Steps:  62%|██████▏   | 246/400 [22:06<13:58,  5.45s/it, lr=1e-5, step_loss=0.122]Steps:  62%|██████▏   | 247/400 [22:11<13:39,  5.35s/it, lr=1e-5, step_loss=0.122]Steps:  62%|██████▏   | 247/400 [22:11<13:39,  5.35s/it, lr=1e-5, step_loss=0.123]Steps:  62%|██████▏   | 248/400 [22:16<13:17,  5.25s/it, lr=1e-5, step_loss=0.123]Steps:  62%|██████▏   | 248/400 [22:16<13:17,  5.25s/it, lr=1e-5, step_loss=0.0792]Steps:  62%|██████▏   | 249/400 [22:20<12:06,  4.81s/it, lr=1e-5, step_loss=0.0792]Steps:  62%|██████▏   | 249/400 [22:20<12:06,  4.81s/it, lr=1e-5, step_loss=0.254] Steps:  62%|██████▎   | 250/400 [22:25<12:14,  4.90s/it, lr=1e-5, step_loss=0.254]Steps:  62%|██████▎   | 250/400 [22:25<12:14,  4.90s/it, lr=1e-5, step_loss=0.153]Steps:  63%|██████▎   | 251/400 [22:31<13:11,  5.31s/it, lr=1e-5, step_loss=0.153]Steps:  63%|██████▎   | 251/400 [22:31<13:11,  5.31s/it, lr=1e-5, step_loss=0.08] Steps:  63%|██████▎   | 252/400 [22:36<12:51,  5.21s/it, lr=1e-5, step_loss=0.08]Steps:  63%|██████▎   | 252/400 [22:36<12:51,  5.21s/it, lr=1e-5, step_loss=0.0795]Steps:  63%|██████▎   | 253/400 [22:41<12:33,  5.13s/it, lr=1e-5, step_loss=0.0795]Steps:  63%|██████▎   | 253/400 [22:41<12:33,  5.13s/it, lr=1e-5, step_loss=0.0502]Steps:  64%|██████▎   | 254/400 [22:46<12:29,  5.13s/it, lr=1e-5, step_loss=0.0502]Steps:  64%|██████▎   | 254/400 [22:46<12:29,  5.13s/it, lr=1e-5, step_loss=0.11]  Steps:  64%|██████▍   | 255/400 [22:51<12:23,  5.13s/it, lr=1e-5, step_loss=0.11]Steps:  64%|██████▍   | 255/400 [22:51<12:23,  5.13s/it, lr=1e-5, step_loss=0.148]Steps:  64%|██████▍   | 256/400 [22:58<13:12,  5.50s/it, lr=1e-5, step_loss=0.148]Steps:  64%|██████▍   | 256/400 [22:58<13:12,  5.50s/it, lr=1e-5, step_loss=0.115]Steps:  64%|██████▍   | 257/400 [23:03<12:51,  5.40s/it, lr=1e-5, step_loss=0.115]Steps:  64%|██████▍   | 257/400 [23:03<12:51,  5.40s/it, lr=1e-5, step_loss=0.0435]Steps:  64%|██████▍   | 258/400 [23:07<11:39,  4.92s/it, lr=1e-5, step_loss=0.0435]Steps:  64%|██████▍   | 258/400 [23:07<11:39,  4.92s/it, lr=1e-5, step_loss=0.208] Steps:  65%|██████▍   | 259/400 [23:12<11:37,  4.94s/it, lr=1e-5, step_loss=0.208]Steps:  65%|██████▍   | 259/400 [23:12<11:37,  4.94s/it, lr=1e-5, step_loss=0.143]Steps:  65%|██████▌   | 260/400 [23:17<11:39,  4.99s/it, lr=1e-5, step_loss=0.143]Steps:  65%|██████▌   | 260/400 [23:17<11:39,  4.99s/it, lr=1e-5, step_loss=0.0625]Steps:  65%|██████▌   | 261/400 [23:23<12:34,  5.43s/it, lr=1e-5, step_loss=0.0625]Steps:  65%|██████▌   | 261/400 [23:23<12:34,  5.43s/it, lr=1e-5, step_loss=0.19]  Steps:  66%|██████▌   | 262/400 [23:28<12:11,  5.30s/it, lr=1e-5, step_loss=0.19]Steps:  66%|██████▌   | 262/400 [23:28<12:11,  5.30s/it, lr=1e-5, step_loss=0.162]Steps:  66%|██████▌   | 263/400 [23:33<11:51,  5.20s/it, lr=1e-5, step_loss=0.162]Steps:  66%|██████▌   | 263/400 [23:33<11:51,  5.20s/it, lr=1e-5, step_loss=0.162]Steps:  66%|██████▌   | 264/400 [23:38<11:44,  5.18s/it, lr=1e-5, step_loss=0.162]Steps:  66%|██████▌   | 264/400 [23:38<11:44,  5.18s/it, lr=1e-5, step_loss=0.0678]Steps:  66%|██████▋   | 265/400 [23:43<11:34,  5.15s/it, lr=1e-5, step_loss=0.0678]Steps:  66%|██████▋   | 265/400 [23:43<11:34,  5.15s/it, lr=1e-5, step_loss=0.0612]Steps:  66%|██████▋   | 266/400 [23:49<12:13,  5.48s/it, lr=1e-5, step_loss=0.0612]Steps:  66%|██████▋   | 266/400 [23:49<12:13,  5.48s/it, lr=1e-5, step_loss=0.0786]Steps:  67%|██████▋   | 267/400 [23:55<11:53,  5.36s/it, lr=1e-5, step_loss=0.0786]Steps:  67%|██████▋   | 267/400 [23:55<11:53,  5.36s/it, lr=1e-5, step_loss=0.127] Steps:  67%|██████▋   | 268/400 [24:00<11:34,  5.26s/it, lr=1e-5, step_loss=0.127]Steps:  67%|██████▋   | 268/400 [24:00<11:34,  5.26s/it, lr=1e-5, step_loss=0.0406]Steps:  67%|██████▋   | 269/400 [24:05<11:17,  5.17s/it, lr=1e-5, step_loss=0.0406]Steps:  67%|██████▋   | 269/400 [24:05<11:17,  5.17s/it, lr=1e-5, step_loss=0.106] Steps:  68%|██████▊   | 270/400 [24:10<11:17,  5.21s/it, lr=1e-5, step_loss=0.106]Steps:  68%|██████▊   | 270/400 [24:10<11:17,  5.21s/it, lr=1e-5, step_loss=0.0598]Steps:  68%|██████▊   | 271/400 [24:16<11:52,  5.52s/it, lr=1e-5, step_loss=0.0598]Steps:  68%|██████▊   | 271/400 [24:16<11:52,  5.52s/it, lr=1e-5, step_loss=0.0898]Steps:  68%|██████▊   | 272/400 [24:21<11:24,  5.35s/it, lr=1e-5, step_loss=0.0898]Steps:  68%|██████▊   | 272/400 [24:21<11:24,  5.35s/it, lr=1e-5, step_loss=0.123] Steps:  68%|██████▊   | 273/400 [24:25<10:20,  4.88s/it, lr=1e-5, step_loss=0.123]Steps:  68%|██████▊   | 273/400 [24:25<10:20,  4.88s/it, lr=1e-5, step_loss=0.202]Steps:  68%|██████▊   | 274/400 [24:30<10:24,  4.96s/it, lr=1e-5, step_loss=0.202]Steps:  68%|██████▊   | 274/400 [24:30<10:24,  4.96s/it, lr=1e-5, step_loss=0.139]Steps:  69%|██████▉   | 275/400 [24:35<10:26,  5.01s/it, lr=1e-5, step_loss=0.139]Steps:  69%|██████▉   | 275/400 [24:35<10:26,  5.01s/it, lr=1e-5, step_loss=0.105]Steps:  69%|██████▉   | 276/400 [24:41<11:06,  5.37s/it, lr=1e-5, step_loss=0.105]Steps:  69%|██████▉   | 276/400 [24:41<11:06,  5.37s/it, lr=1e-5, step_loss=0.0599]Steps:  69%|██████▉   | 277/400 [24:47<10:52,  5.31s/it, lr=1e-5, step_loss=0.0599]Steps:  69%|██████▉   | 277/400 [24:47<10:52,  5.31s/it, lr=1e-5, step_loss=0.197] Steps:  70%|██████▉   | 278/400 [24:51<10:34,  5.20s/it, lr=1e-5, step_loss=0.197]Steps:  70%|██████▉   | 278/400 [24:51<10:34,  5.20s/it, lr=1e-5, step_loss=0.0619]Steps:  70%|██████▉   | 279/400 [24:56<10:20,  5.13s/it, lr=1e-5, step_loss=0.0619]Steps:  70%|██████▉   | 279/400 [24:56<10:20,  5.13s/it, lr=1e-5, step_loss=0.166] Steps:  70%|███████   | 280/400 [25:02<10:22,  5.18s/it, lr=1e-5, step_loss=0.166]Steps:  70%|███████   | 280/400 [25:02<10:22,  5.18s/it, lr=1e-5, step_loss=0.116]Steps:  70%|███████   | 281/400 [25:08<10:49,  5.45s/it, lr=1e-5, step_loss=0.116]Steps:  70%|███████   | 281/400 [25:08<10:49,  5.45s/it, lr=1e-5, step_loss=0.119]Steps:  70%|███████   | 282/400 [25:13<10:27,  5.32s/it, lr=1e-5, step_loss=0.119]Steps:  70%|███████   | 282/400 [25:13<10:27,  5.32s/it, lr=1e-5, step_loss=0.103]Steps:  71%|███████   | 283/400 [25:18<10:17,  5.28s/it, lr=1e-5, step_loss=0.103]Steps:  71%|███████   | 283/400 [25:18<10:17,  5.28s/it, lr=1e-5, step_loss=0.137]Steps:  71%|███████   | 284/400 [25:23<10:01,  5.18s/it, lr=1e-5, step_loss=0.137]Steps:  71%|███████   | 284/400 [25:23<10:01,  5.18s/it, lr=1e-5, step_loss=0.0759]Steps:  71%|███████▏  | 285/400 [25:28<09:53,  5.16s/it, lr=1e-5, step_loss=0.0759]Steps:  71%|███████▏  | 285/400 [25:28<09:53,  5.16s/it, lr=1e-5, step_loss=0.0606]Steps:  72%|███████▏  | 286/400 [25:34<10:31,  5.54s/it, lr=1e-5, step_loss=0.0606]Steps:  72%|███████▏  | 286/400 [25:35<10:31,  5.54s/it, lr=1e-5, step_loss=0.11]  Steps:  72%|███████▏  | 287/400 [25:39<10:07,  5.37s/it, lr=1e-5, step_loss=0.11]Steps:  72%|███████▏  | 287/400 [25:39<10:07,  5.37s/it, lr=1e-5, step_loss=0.182]Steps:  72%|███████▏  | 288/400 [25:44<09:49,  5.26s/it, lr=1e-5, step_loss=0.182]Steps:  72%|███████▏  | 288/400 [25:44<09:49,  5.26s/it, lr=1e-5, step_loss=0.139]Steps:  72%|███████▏  | 289/400 [25:50<09:39,  5.22s/it, lr=1e-5, step_loss=0.139]Steps:  72%|███████▏  | 289/400 [25:50<09:39,  5.22s/it, lr=1e-5, step_loss=0.0712]Steps:  72%|███████▎  | 290/400 [25:55<09:30,  5.18s/it, lr=1e-5, step_loss=0.0712]Steps:  72%|███████▎  | 290/400 [25:55<09:30,  5.18s/it, lr=1e-5, step_loss=0.118] Steps:  73%|███████▎  | 291/400 [26:01<09:57,  5.48s/it, lr=1e-5, step_loss=0.118]Steps:  73%|███████▎  | 291/400 [26:01<09:57,  5.48s/it, lr=1e-5, step_loss=0.101]Steps:  73%|███████▎  | 292/400 [26:06<09:40,  5.37s/it, lr=1e-5, step_loss=0.101]Steps:  73%|███████▎  | 292/400 [26:06<09:40,  5.37s/it, lr=1e-5, step_loss=0.0866]Steps:  73%|███████▎  | 293/400 [26:11<09:21,  5.25s/it, lr=1e-5, step_loss=0.0866]Steps:  73%|███████▎  | 293/400 [26:11<09:21,  5.25s/it, lr=1e-5, step_loss=0.119] Steps:  74%|███████▎  | 294/400 [26:16<09:07,  5.16s/it, lr=1e-5, step_loss=0.119]Steps:  74%|███████▎  | 294/400 [26:16<09:07,  5.16s/it, lr=1e-5, step_loss=0.109]Steps:  74%|███████▍  | 295/400 [26:21<09:06,  5.20s/it, lr=1e-5, step_loss=0.109]Steps:  74%|███████▍  | 295/400 [26:21<09:06,  5.20s/it, lr=1e-5, step_loss=0.0789]Steps:  74%|███████▍  | 296/400 [26:27<09:30,  5.49s/it, lr=1e-5, step_loss=0.0789]Steps:  74%|███████▍  | 296/400 [26:27<09:30,  5.49s/it, lr=1e-5, step_loss=0.118] Steps:  74%|███████▍  | 297/400 [26:32<09:08,  5.33s/it, lr=1e-5, step_loss=0.118]Steps:  74%|███████▍  | 297/400 [26:32<09:08,  5.33s/it, lr=1e-5, step_loss=0.077]Steps:  74%|███████▍  | 298/400 [26:37<08:57,  5.27s/it, lr=1e-5, step_loss=0.077]Steps:  74%|███████▍  | 298/400 [26:37<08:57,  5.27s/it, lr=1e-5, step_loss=0.081]Steps:  75%|███████▍  | 299/400 [26:42<08:43,  5.18s/it, lr=1e-5, step_loss=0.081]Steps:  75%|███████▍  | 299/400 [26:42<08:43,  5.18s/it, lr=1e-5, step_loss=0.0975]Steps:  75%|███████▌  | 300/400 [26:48<08:36,  5.16s/it, lr=1e-5, step_loss=0.0975]Steps:  75%|███████▌  | 300/400 [26:48<08:36,  5.16s/it, lr=1e-5, step_loss=0.101] Steps:  75%|███████▌  | 301/400 [26:54<09:09,  5.55s/it, lr=1e-5, step_loss=0.101]Steps:  75%|███████▌  | 301/400 [26:54<09:09,  5.55s/it, lr=1e-5, step_loss=0.084]Steps:  76%|███████▌  | 302/400 [26:59<08:44,  5.36s/it, lr=1e-5, step_loss=0.084]Steps:  76%|███████▌  | 302/400 [26:59<08:44,  5.36s/it, lr=1e-5, step_loss=0.0765]Steps:  76%|███████▌  | 303/400 [27:04<08:28,  5.25s/it, lr=1e-5, step_loss=0.0765]Steps:  76%|███████▌  | 303/400 [27:04<08:28,  5.25s/it, lr=1e-5, step_loss=0.115] Steps:  76%|███████▌  | 304/400 [27:09<08:20,  5.21s/it, lr=1e-5, step_loss=0.115]Steps:  76%|███████▌  | 304/400 [27:09<08:20,  5.21s/it, lr=1e-5, step_loss=0.116]Steps:  76%|███████▋  | 305/400 [27:14<08:12,  5.19s/it, lr=1e-5, step_loss=0.116]Steps:  76%|███████▋  | 305/400 [27:14<08:12,  5.19s/it, lr=1e-5, step_loss=0.165]Steps:  76%|███████▋  | 306/400 [27:20<08:30,  5.43s/it, lr=1e-5, step_loss=0.165]Steps:  76%|███████▋  | 306/400 [27:20<08:30,  5.43s/it, lr=1e-5, step_loss=0.069]Steps:  77%|███████▋  | 307/400 [27:25<08:18,  5.36s/it, lr=1e-5, step_loss=0.069]Steps:  77%|███████▋  | 307/400 [27:25<08:18,  5.36s/it, lr=1e-5, step_loss=0.113]Steps:  77%|███████▋  | 308/400 [27:30<08:02,  5.24s/it, lr=1e-5, step_loss=0.113]Steps:  77%|███████▋  | 308/400 [27:30<08:02,  5.24s/it, lr=1e-5, step_loss=0.177]Steps:  77%|███████▋  | 309/400 [27:35<07:50,  5.17s/it, lr=1e-5, step_loss=0.177]Steps:  77%|███████▋  | 309/400 [27:35<07:50,  5.17s/it, lr=1e-5, step_loss=0.0771]Steps:  78%|███████▊  | 310/400 [27:41<07:48,  5.21s/it, lr=1e-5, step_loss=0.0771]Steps:  78%|███████▊  | 310/400 [27:41<07:48,  5.21s/it, lr=1e-5, step_loss=0.0872]Steps:  78%|███████▊  | 311/400 [27:47<08:11,  5.52s/it, lr=1e-5, step_loss=0.0872]Steps:  78%|███████▊  | 311/400 [27:47<08:11,  5.52s/it, lr=1e-5, step_loss=0.131] Steps:  78%|███████▊  | 312/400 [27:52<07:52,  5.37s/it, lr=1e-5, step_loss=0.131]Steps:  78%|███████▊  | 312/400 [27:52<07:52,  5.37s/it, lr=1e-5, step_loss=0.121]Steps:  78%|███████▊  | 313/400 [27:57<07:39,  5.28s/it, lr=1e-5, step_loss=0.121]Steps:  78%|███████▊  | 313/400 [27:57<07:39,  5.28s/it, lr=1e-5, step_loss=0.0781]Steps:  78%|███████▊  | 314/400 [28:02<07:26,  5.19s/it, lr=1e-5, step_loss=0.0781]Steps:  78%|███████▊  | 314/400 [28:02<07:26,  5.19s/it, lr=1e-5, step_loss=0.0812]Steps:  79%|███████▉  | 315/400 [28:07<07:20,  5.18s/it, lr=1e-5, step_loss=0.0812]Steps:  79%|███████▉  | 315/400 [28:07<07:20,  5.18s/it, lr=1e-5, step_loss=0.0999]07/03/2024 07:36:31 - INFO - __main__ - 3 checkpoints already exist, removing 1 checkpoints
07/03/2024 07:36:31 - INFO - __main__ - removing checkpoints: reference_unet-105.pth
07/03/2024 07:36:37 - INFO - __main__ - 3 checkpoints already exist, removing 1 checkpoints
07/03/2024 07:36:37 - INFO - __main__ - removing checkpoints: denoising_unet-105.pth
07/03/2024 07:36:48 - INFO - __main__ - 3 checkpoints already exist, removing 1 checkpoints
07/03/2024 07:36:48 - INFO - __main__ - removing checkpoints: pose_guider-105.pth
Steps:  79%|███████▉  | 316/400 [28:40<18:46, 13.41s/it, lr=1e-5, step_loss=0.0999]Steps:  79%|███████▉  | 316/400 [28:40<18:46, 13.41s/it, lr=1e-5, step_loss=0.125] Steps:  79%|███████▉  | 317/400 [28:44<14:34, 10.53s/it, lr=1e-5, step_loss=0.125]Steps:  79%|███████▉  | 317/400 [28:44<14:34, 10.53s/it, lr=1e-5, step_loss=0.129]Steps:  80%|███████▉  | 318/400 [28:47<11:38,  8.52s/it, lr=1e-5, step_loss=0.129]Steps:  80%|███████▉  | 318/400 [28:47<11:38,  8.52s/it, lr=1e-5, step_loss=0.188]Steps:  80%|███████▉  | 319/400 [28:52<10:05,  7.47s/it, lr=1e-5, step_loss=0.188]Steps:  80%|███████▉  | 319/400 [28:52<10:05,  7.47s/it, lr=1e-5, step_loss=0.0802]Steps:  80%|████████  | 320/400 [28:58<09:01,  6.77s/it, lr=1e-5, step_loss=0.0802]Steps:  80%|████████  | 320/400 [28:58<09:01,  6.77s/it, lr=1e-5, step_loss=0.123] Steps:  80%|████████  | 321/400 [29:04<08:40,  6.59s/it, lr=1e-5, step_loss=0.123]Steps:  80%|████████  | 321/400 [29:04<08:40,  6.59s/it, lr=1e-5, step_loss=0.146]Steps:  80%|████████  | 322/400 [29:09<07:57,  6.12s/it, lr=1e-5, step_loss=0.146]Steps:  80%|████████  | 322/400 [29:09<07:57,  6.12s/it, lr=1e-5, step_loss=0.0892]Steps:  81%|████████  | 323/400 [29:14<07:24,  5.78s/it, lr=1e-5, step_loss=0.0892]Steps:  81%|████████  | 323/400 [29:14<07:24,  5.78s/it, lr=1e-5, step_loss=0.073] Steps:  81%|████████  | 324/400 [29:19<07:04,  5.58s/it, lr=1e-5, step_loss=0.073]Steps:  81%|████████  | 324/400 [29:19<07:04,  5.58s/it, lr=1e-5, step_loss=0.127]Steps:  81%|████████▏ | 325/400 [29:24<06:48,  5.45s/it, lr=1e-5, step_loss=0.127]Steps:  81%|████████▏ | 325/400 [29:24<06:48,  5.45s/it, lr=1e-5, step_loss=0.0789]Steps:  82%|████████▏ | 326/400 [29:30<07:00,  5.68s/it, lr=1e-5, step_loss=0.0789]Steps:  82%|████████▏ | 326/400 [29:30<07:00,  5.68s/it, lr=1e-5, step_loss=0.0907]Steps:  82%|████████▏ | 327/400 [29:35<06:42,  5.51s/it, lr=1e-5, step_loss=0.0907]Steps:  82%|████████▏ | 327/400 [29:35<06:42,  5.51s/it, lr=1e-5, step_loss=0.117] Steps:  82%|████████▏ | 328/400 [29:40<06:25,  5.35s/it, lr=1e-5, step_loss=0.117]Steps:  82%|████████▏ | 328/400 [29:40<06:25,  5.35s/it, lr=1e-5, step_loss=0.0861]Steps:  82%|████████▏ | 329/400 [29:45<06:11,  5.23s/it, lr=1e-5, step_loss=0.0861]Steps:  82%|████████▏ | 329/400 [29:45<06:11,  5.23s/it, lr=1e-5, step_loss=0.0608]Steps:  82%|████████▎ | 330/400 [29:51<06:07,  5.25s/it, lr=1e-5, step_loss=0.0608]Steps:  82%|████████▎ | 330/400 [29:51<06:07,  5.25s/it, lr=1e-5, step_loss=0.104] Steps:  83%|████████▎ | 331/400 [29:57<06:21,  5.53s/it, lr=1e-5, step_loss=0.104]Steps:  83%|████████▎ | 331/400 [29:57<06:21,  5.53s/it, lr=1e-5, step_loss=0.0527]Steps:  83%|████████▎ | 332/400 [30:02<06:04,  5.36s/it, lr=1e-5, step_loss=0.0527]Steps:  83%|████████▎ | 332/400 [30:02<06:04,  5.36s/it, lr=1e-5, step_loss=0.107] Steps:  83%|████████▎ | 333/400 [30:07<05:54,  5.29s/it, lr=1e-5, step_loss=0.107]Steps:  83%|████████▎ | 333/400 [30:07<05:54,  5.29s/it, lr=1e-5, step_loss=0.0787]Steps:  84%|████████▎ | 334/400 [30:12<05:42,  5.19s/it, lr=1e-5, step_loss=0.0787]Steps:  84%|████████▎ | 334/400 [30:12<05:42,  5.19s/it, lr=1e-5, step_loss=0.0899]Steps:  84%|████████▍ | 335/400 [30:16<05:13,  4.82s/it, lr=1e-5, step_loss=0.0899]Steps:  84%|████████▍ | 335/400 [30:16<05:13,  4.82s/it, lr=1e-5, step_loss=0.502] Steps:  84%|████████▍ | 336/400 [30:22<05:36,  5.26s/it, lr=1e-5, step_loss=0.502]Steps:  84%|████████▍ | 336/400 [30:22<05:36,  5.26s/it, lr=1e-5, step_loss=0.135]Steps:  84%|████████▍ | 337/400 [30:27<05:28,  5.22s/it, lr=1e-5, step_loss=0.135]Steps:  84%|████████▍ | 337/400 [30:27<05:28,  5.22s/it, lr=1e-5, step_loss=0.088]Steps:  84%|████████▍ | 338/400 [30:32<05:19,  5.15s/it, lr=1e-5, step_loss=0.088]Steps:  84%|████████▍ | 338/400 [30:32<05:19,  5.15s/it, lr=1e-5, step_loss=0.0658]Steps:  85%|████████▍ | 339/400 [30:36<04:49,  4.74s/it, lr=1e-5, step_loss=0.0658]Steps:  85%|████████▍ | 339/400 [30:36<04:49,  4.74s/it, lr=1e-5, step_loss=0.103] Steps:  85%|████████▌ | 340/400 [30:40<04:29,  4.50s/it, lr=1e-5, step_loss=0.103]Steps:  85%|████████▌ | 340/400 [30:40<04:29,  4.50s/it, lr=1e-5, step_loss=0.068]Steps:  85%|████████▌ | 341/400 [30:46<04:55,  5.01s/it, lr=1e-5, step_loss=0.068]Steps:  85%|████████▌ | 341/400 [30:46<04:55,  5.01s/it, lr=1e-5, step_loss=0.0939]Steps:  86%|████████▌ | 342/400 [30:51<04:52,  5.04s/it, lr=1e-5, step_loss=0.0939]Steps:  86%|████████▌ | 342/400 [30:51<04:52,  5.04s/it, lr=1e-5, step_loss=0.11]  Steps:  86%|████████▌ | 343/400 [30:56<04:45,  5.02s/it, lr=1e-5, step_loss=0.11]Steps:  86%|████████▌ | 343/400 [30:56<04:45,  5.02s/it, lr=1e-5, step_loss=0.107]Steps:  86%|████████▌ | 344/400 [31:01<04:40,  5.00s/it, lr=1e-5, step_loss=0.107]Steps:  86%|████████▌ | 344/400 [31:01<04:40,  5.00s/it, lr=1e-5, step_loss=0.0755]Steps:  86%|████████▋ | 345/400 [31:06<04:39,  5.08s/it, lr=1e-5, step_loss=0.0755]Steps:  86%|████████▋ | 345/400 [31:06<04:39,  5.08s/it, lr=1e-5, step_loss=0.0722]Steps:  86%|████████▋ | 346/400 [31:13<04:52,  5.41s/it, lr=1e-5, step_loss=0.0722]Steps:  86%|████████▋ | 346/400 [31:13<04:52,  5.41s/it, lr=1e-5, step_loss=0.111] Steps:  87%|████████▋ | 347/400 [31:18<04:40,  5.28s/it, lr=1e-5, step_loss=0.111]Steps:  87%|████████▋ | 347/400 [31:18<04:40,  5.28s/it, lr=1e-5, step_loss=0.0777]Steps:  87%|████████▋ | 348/400 [31:21<04:11,  4.84s/it, lr=1e-5, step_loss=0.0777]Steps:  87%|████████▋ | 348/400 [31:21<04:11,  4.84s/it, lr=1e-5, step_loss=0.179] Steps:  87%|████████▋ | 349/400 [31:26<04:12,  4.94s/it, lr=1e-5, step_loss=0.179]Steps:  87%|████████▋ | 349/400 [31:27<04:12,  4.94s/it, lr=1e-5, step_loss=0.0968]Steps:  88%|████████▊ | 350/400 [31:32<04:09,  5.00s/it, lr=1e-5, step_loss=0.0968]Steps:  88%|████████▊ | 350/400 [31:32<04:09,  5.00s/it, lr=1e-5, step_loss=0.0639]Steps:  88%|████████▊ | 351/400 [31:38<04:23,  5.39s/it, lr=1e-5, step_loss=0.0639]Steps:  88%|████████▊ | 351/400 [31:38<04:23,  5.39s/it, lr=1e-5, step_loss=0.123] Steps:  88%|████████▊ | 352/400 [31:43<04:15,  5.33s/it, lr=1e-5, step_loss=0.123]Steps:  88%|████████▊ | 352/400 [31:43<04:15,  5.33s/it, lr=1e-5, step_loss=0.076]Steps:  88%|████████▊ | 353/400 [31:48<04:05,  5.22s/it, lr=1e-5, step_loss=0.076]Steps:  88%|████████▊ | 353/400 [31:48<04:05,  5.22s/it, lr=1e-5, step_loss=0.0895]Steps:  88%|████████▊ | 354/400 [31:53<03:56,  5.15s/it, lr=1e-5, step_loss=0.0895]Steps:  88%|████████▊ | 354/400 [31:53<03:56,  5.15s/it, lr=1e-5, step_loss=0.0992]Steps:  89%|████████▉ | 355/400 [31:58<03:53,  5.20s/it, lr=1e-5, step_loss=0.0992]Steps:  89%|████████▉ | 355/400 [31:58<03:53,  5.20s/it, lr=1e-5, step_loss=0.0943]Steps:  89%|████████▉ | 356/400 [32:05<04:01,  5.49s/it, lr=1e-5, step_loss=0.0943]Steps:  89%|████████▉ | 356/400 [32:05<04:01,  5.49s/it, lr=1e-5, step_loss=0.0924]Steps:  89%|████████▉ | 357/400 [32:10<03:49,  5.33s/it, lr=1e-5, step_loss=0.0924]Steps:  89%|████████▉ | 357/400 [32:10<03:49,  5.33s/it, lr=1e-5, step_loss=0.0562]Steps:  90%|████████▉ | 358/400 [32:15<03:41,  5.27s/it, lr=1e-5, step_loss=0.0562]Steps:  90%|████████▉ | 358/400 [32:15<03:41,  5.27s/it, lr=1e-5, step_loss=0.0877]Steps:  90%|████████▉ | 359/400 [32:20<03:32,  5.18s/it, lr=1e-5, step_loss=0.0877]Steps:  90%|████████▉ | 359/400 [32:20<03:32,  5.18s/it, lr=1e-5, step_loss=0.122] Steps:  90%|█████████ | 360/400 [32:25<03:26,  5.16s/it, lr=1e-5, step_loss=0.122]Steps:  90%|█████████ | 360/400 [32:25<03:26,  5.16s/it, lr=1e-5, step_loss=0.0568]Steps:  90%|█████████ | 361/400 [32:31<03:35,  5.52s/it, lr=1e-5, step_loss=0.0568]Steps:  90%|█████████ | 361/400 [32:31<03:35,  5.52s/it, lr=1e-5, step_loss=0.0698]Steps:  90%|█████████ | 362/400 [32:36<03:23,  5.35s/it, lr=1e-5, step_loss=0.0698]Steps:  90%|█████████ | 362/400 [32:36<03:23,  5.35s/it, lr=1e-5, step_loss=0.0698]Steps:  91%|█████████ | 363/400 [32:41<03:13,  5.22s/it, lr=1e-5, step_loss=0.0698]Steps:  91%|█████████ | 363/400 [32:41<03:13,  5.22s/it, lr=1e-5, step_loss=0.0943]Steps:  91%|█████████ | 364/400 [32:46<03:07,  5.20s/it, lr=1e-5, step_loss=0.0943]Steps:  91%|█████████ | 364/400 [32:46<03:07,  5.20s/it, lr=1e-5, step_loss=0.0785]Steps:  91%|█████████▏| 365/400 [32:51<03:01,  5.18s/it, lr=1e-5, step_loss=0.0785]Steps:  91%|█████████▏| 365/400 [32:51<03:01,  5.18s/it, lr=1e-5, step_loss=0.0504]Steps:  92%|█████████▏| 366/400 [32:56<02:54,  5.14s/it, lr=1e-5, step_loss=0.0504]Steps:  92%|█████████▏| 366/400 [32:56<02:54,  5.14s/it, lr=1e-5, step_loss=0.119] Steps:  92%|█████████▏| 367/400 [33:01<02:47,  5.07s/it, lr=1e-5, step_loss=0.119]Steps:  92%|█████████▏| 367/400 [33:01<02:47,  5.07s/it, lr=1e-5, step_loss=0.0653]Steps:  92%|█████████▏| 368/400 [33:06<02:42,  5.09s/it, lr=1e-5, step_loss=0.0653]Steps:  92%|█████████▏| 368/400 [33:06<02:42,  5.09s/it, lr=1e-5, step_loss=0.0944]Steps:  92%|█████████▏| 369/400 [33:11<02:37,  5.07s/it, lr=1e-5, step_loss=0.0944]Steps:  92%|█████████▏| 369/400 [33:11<02:37,  5.07s/it, lr=1e-5, step_loss=0.0703]Steps:  92%|█████████▎| 370/400 [33:16<02:32,  5.09s/it, lr=1e-5, step_loss=0.0703]Steps:  92%|█████████▎| 370/400 [33:16<02:32,  5.09s/it, lr=1e-5, step_loss=0.0786]Steps:  93%|█████████▎| 371/400 [33:23<02:38,  5.47s/it, lr=1e-5, step_loss=0.0786]Steps:  93%|█████████▎| 371/400 [33:23<02:38,  5.47s/it, lr=1e-5, step_loss=0.0902]Steps:  93%|█████████▎| 372/400 [33:28<02:28,  5.31s/it, lr=1e-5, step_loss=0.0902]Steps:  93%|█████████▎| 372/400 [33:28<02:28,  5.31s/it, lr=1e-5, step_loss=0.0986]Steps:  93%|█████████▎| 373/400 [33:33<02:20,  5.20s/it, lr=1e-5, step_loss=0.0986]Steps:  93%|█████████▎| 373/400 [33:33<02:20,  5.20s/it, lr=1e-5, step_loss=0.111] Steps:  94%|█████████▎| 374/400 [33:37<02:04,  4.78s/it, lr=1e-5, step_loss=0.111]Steps:  94%|█████████▎| 374/400 [33:37<02:04,  4.78s/it, lr=1e-5, step_loss=0.245]Steps:  94%|█████████▍| 375/400 [33:42<02:03,  4.93s/it, lr=1e-5, step_loss=0.245]Steps:  94%|█████████▍| 375/400 [33:42<02:03,  4.93s/it, lr=1e-5, step_loss=0.0787]Steps:  94%|█████████▍| 376/400 [33:48<02:08,  5.36s/it, lr=1e-5, step_loss=0.0787]Steps:  94%|█████████▍| 376/400 [33:48<02:08,  5.36s/it, lr=1e-5, step_loss=0.0636]Steps:  94%|█████████▍| 377/400 [33:53<02:00,  5.24s/it, lr=1e-5, step_loss=0.0636]Steps:  94%|█████████▍| 377/400 [33:53<02:00,  5.24s/it, lr=1e-5, step_loss=0.0904]Steps:  94%|█████████▍| 378/400 [33:58<01:54,  5.21s/it, lr=1e-5, step_loss=0.0904]Steps:  94%|█████████▍| 378/400 [33:58<01:54,  5.21s/it, lr=1e-5, step_loss=0.0665]Steps:  95%|█████████▍| 379/400 [34:02<01:40,  4.79s/it, lr=1e-5, step_loss=0.0665]Steps:  95%|█████████▍| 379/400 [34:02<01:40,  4.79s/it, lr=1e-5, step_loss=0.0763]Steps:  95%|█████████▌| 380/400 [34:07<01:37,  4.87s/it, lr=1e-5, step_loss=0.0763]Steps:  95%|█████████▌| 380/400 [34:07<01:37,  4.87s/it, lr=1e-5, step_loss=0.138] Steps:  95%|█████████▌| 381/400 [34:13<01:39,  5.24s/it, lr=1e-5, step_loss=0.138]Steps:  95%|█████████▌| 381/400 [34:13<01:39,  5.24s/it, lr=1e-5, step_loss=0.0525]Steps:  96%|█████████▌| 382/400 [34:18<01:33,  5.20s/it, lr=1e-5, step_loss=0.0525]Steps:  96%|█████████▌| 382/400 [34:18<01:33,  5.20s/it, lr=1e-5, step_loss=0.091] Steps:  96%|█████████▌| 383/400 [34:23<01:27,  5.13s/it, lr=1e-5, step_loss=0.091]Steps:  96%|█████████▌| 383/400 [34:23<01:27,  5.13s/it, lr=1e-5, step_loss=0.113]Steps:  96%|█████████▌| 384/400 [34:28<01:21,  5.08s/it, lr=1e-5, step_loss=0.113]Steps:  96%|█████████▌| 384/400 [34:28<01:21,  5.08s/it, lr=1e-5, step_loss=0.0435]Steps:  96%|█████████▋| 385/400 [34:34<01:17,  5.16s/it, lr=1e-5, step_loss=0.0435]Steps:  96%|█████████▋| 385/400 [34:34<01:17,  5.16s/it, lr=1e-5, step_loss=0.0699]Steps:  96%|█████████▋| 386/400 [34:40<01:16,  5.49s/it, lr=1e-5, step_loss=0.0699]Steps:  96%|█████████▋| 386/400 [34:40<01:16,  5.49s/it, lr=1e-5, step_loss=0.0748]Steps:  97%|█████████▋| 387/400 [34:45<01:09,  5.34s/it, lr=1e-5, step_loss=0.0748]Steps:  97%|█████████▋| 387/400 [34:45<01:09,  5.34s/it, lr=1e-5, step_loss=0.109] Steps:  97%|█████████▋| 388/400 [34:50<01:03,  5.27s/it, lr=1e-5, step_loss=0.109]Steps:  97%|█████████▋| 388/400 [34:50<01:03,  5.27s/it, lr=1e-5, step_loss=0.0909]Steps:  97%|█████████▋| 389/400 [34:55<00:56,  5.17s/it, lr=1e-5, step_loss=0.0909]Steps:  97%|█████████▋| 389/400 [34:55<00:56,  5.17s/it, lr=1e-5, step_loss=0.097] Steps:  98%|█████████▊| 390/400 [35:00<00:51,  5.15s/it, lr=1e-5, step_loss=0.097]Steps:  98%|█████████▊| 390/400 [35:00<00:51,  5.15s/it, lr=1e-5, step_loss=0.105]Steps:  98%|█████████▊| 391/400 [35:06<00:49,  5.54s/it, lr=1e-5, step_loss=0.105]Steps:  98%|█████████▊| 391/400 [35:06<00:49,  5.54s/it, lr=1e-5, step_loss=0.0733]Steps:  98%|█████████▊| 392/400 [35:11<00:43,  5.38s/it, lr=1e-5, step_loss=0.0733]Steps:  98%|█████████▊| 392/400 [35:11<00:43,  5.38s/it, lr=1e-5, step_loss=0.0653]Steps:  98%|█████████▊| 393/400 [35:16<00:36,  5.23s/it, lr=1e-5, step_loss=0.0653]Steps:  98%|█████████▊| 393/400 [35:16<00:36,  5.23s/it, lr=1e-5, step_loss=0.102] Steps:  98%|█████████▊| 394/400 [35:22<00:31,  5.21s/it, lr=1e-5, step_loss=0.102]Steps:  98%|█████████▊| 394/400 [35:22<00:31,  5.21s/it, lr=1e-5, step_loss=0.0631]Steps:  99%|█████████▉| 395/400 [35:27<00:25,  5.20s/it, lr=1e-5, step_loss=0.0631]Steps:  99%|█████████▉| 395/400 [35:27<00:25,  5.20s/it, lr=1e-5, step_loss=0.104] Steps:  99%|█████████▉| 396/400 [35:33<00:21,  5.50s/it, lr=1e-5, step_loss=0.104]Steps:  99%|█████████▉| 396/400 [35:33<00:21,  5.50s/it, lr=1e-5, step_loss=0.0667]Steps:  99%|█████████▉| 397/400 [35:38<00:16,  5.39s/it, lr=1e-5, step_loss=0.0667]Steps:  99%|█████████▉| 397/400 [35:38<00:16,  5.39s/it, lr=1e-5, step_loss=0.128] Steps: 100%|█████████▉| 398/400 [35:43<00:10,  5.26s/it, lr=1e-5, step_loss=0.128]Steps: 100%|█████████▉| 398/400 [35:43<00:10,  5.26s/it, lr=1e-5, step_loss=0.0413]Steps: 100%|█████████▉| 399/400 [35:48<00:05,  5.18s/it, lr=1e-5, step_loss=0.0413]Steps: 100%|█████████▉| 399/400 [35:48<00:05,  5.18s/it, lr=1e-5, step_loss=0.098] Steps: 100%|██████████| 400/400 [35:53<00:00,  5.21s/it, lr=1e-5, step_loss=0.098]Steps: 100%|██████████| 400/400 [35:53<00:00,  5.21s/it, lr=1e-5, step_loss=0.0671]Steps: 100%|██████████| 400/400 [35:53<00:00,  5.38s/it, lr=1e-5, step_loss=0.0671]
